{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256d845c-9397-4d6f-92e4-18fef599770c",
   "metadata": {},
   "source": [
    "# Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f7903b-7bbe-41b6-a21b-e73ede0ab03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "import random\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c5f0e-e677-476a-ab59-efd7bafe9a97",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff81d3-b8e2-4d85-a771-35fa34f2ba0e",
   "metadata": {},
   "source": [
    "In case this needs to be said, if you're using this approach to solve a problem prominent to you, don't bother generating a dataset, use the one you're working with. I'm only generating a dataset for simplicity, reproducibility and to outline a use case on how such a problem could be solved using active learning.  \n",
    "\n",
    "We are going to generate fake vital statistics associated to humans. The function we're building will randomly generate fake height, weight and age data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6428589d-7938-4efd-86b7-72d3d20bb0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n = 1000):\n",
    "    '''\n",
    "    This function will simulate the generation of gender data. It will randomly create\n",
    "    columns associated to the height (cm), weight, age and gender. The gender column\n",
    "    will not be completely filled out, it will have a few sample rows labelled to a gender.\n",
    "    \n",
    "    params:\n",
    "        n (Integer) : The number of rows you want to synthesize\n",
    "        \n",
    "    returns:\n",
    "        A dataframe with the columns of uuid, height, weight, age and gender.\n",
    "            - uuid (UUID4) : A unique identifier to the user\n",
    "            - height (Integer) : The height of the user in cm\n",
    "            - weight (Integer) : The weight of the user in pounds (lbs)\n",
    "            - age (Integer) : The age of the user\n",
    "            - gender (String) : The gender associated to the user if known\n",
    "        \n",
    "    example:\n",
    "        gender_df = generate_data(n = 1000)\n",
    "    '''\n",
    "    # we have more np.nan than Male or Female so that we can skew majority of the\n",
    "    # data to be missing\n",
    "    genders = ['Male', 'Female', np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "    height_range = (50, 200)\n",
    "    weight_range = (30, 250)\n",
    "    age_range = (3, 70)\n",
    "    \n",
    "    d = pd.DataFrame(\n",
    "        {\n",
    "            'uuid' : [uuid.uuid4() for _ in range(n)],\n",
    "            'height' : [random.randint(height_range[0], height_range[1]) for _ in range(n)],\n",
    "            'weight' : [random.randint(weight_range[0], weight_range[1]) for _ in range(n)],\n",
    "            'age' : [random.randint(age_range[0], age_range[1]) for _ in range(n)],\n",
    "            'gender' : [random.choice(genders) for _ in range(n)]\n",
    "        }\n",
    "    ).drop_duplicates()\n",
    "    d = d.set_index('uuid')\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065c30ec-3803-463d-befe-61c453c0f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_df = generate_data(n = 1000)\n",
    "gender_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff96ce2-5987-477e-9cad-5d21acc406ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4182d15c-9f74-4b74-b549-2cf600f8346d",
   "metadata": {},
   "source": [
    "Of course, since we randomly generated data, we will have weird labelled values, like a 39 year old man who weighs 214 pounds and is 78 centimeters tall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba120b-8c0a-4aab-abba-05f3071d3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f15786b-0b9b-4ff9-bd22-44824342a02f",
   "metadata": {},
   "source": [
    "## Annotate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038f443b-36e7-41c2-80ac-cf4a06050487",
   "metadata": {},
   "source": [
    "Here people would usually annotate their datasets associated to the labels, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c384ca-d0d8-48e7-97be-f8461ca0ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ae5d8-5982-451c-953f-b7372b998076",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = gender_df[~gender_df['gender'].isna()].copy()\n",
    "pred_df = gender_df[gender_df['gender'].isna()].copy()\n",
    "print(train_df.shape, pred_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db92866c-d2b9-487e-a041-621a35471013",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017f4fb-8f12-444f-b76c-91d4c4ab0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature, target breakdown\n",
    "ft_cols = train_df.drop(columns = ['gender']).columns.tolist()\n",
    "target_col = 'gender'\n",
    "\n",
    "# train test split\n",
    "x = train_df[ft_cols].values\n",
    "y = train_df[target_col].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, \n",
    "    y,\n",
    "    test_size = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3920e1-cca0-4286-a218-5a0af3617311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBC classifier\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "# train the model\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed10e0-82df-4d0e-be2c-16a55ededf02",
   "metadata": {},
   "source": [
    "## Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e0ecc-8d0f-462b-8ac5-18998cc3bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_eval(clf, x_test, y_test):\n",
    "    '''\n",
    "    This function will evaluate a sk-learn multi-class classification model based on its\n",
    "    x_test and y_test values\n",
    "    \n",
    "    params:\n",
    "        clf (Model) : The model you wish to evaluate the performance of\n",
    "        x_test (Array) : Result of the train test split\n",
    "        y_test (Array) : Result of the train test split\n",
    "    \n",
    "    returns:\n",
    "        This function will return the following evaluation metrics:\n",
    "            - Accuracy Score\n",
    "            - Matthews Correlation Coefficient\n",
    "            - Classification Report\n",
    "            - Confusion Matrix\n",
    "    \n",
    "    example:\n",
    "        clf_eval(\n",
    "            clf,\n",
    "            x_test,\n",
    "            y_test\n",
    "        )\n",
    "    '''\n",
    "    y_pred = clf.predict(x_test)\n",
    "    y_true = y_test\n",
    "    \n",
    "    y_pred = clf.predict(x_test)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"Testing Accuracy : \", test_acc)\n",
    "    \n",
    "    print(\"MCC Score : \", matthews_corrcoef(y_true, y_pred))\n",
    "    \n",
    "    print(\"Classification Report : \")\n",
    "    print(classification_report(y_test, clf.predict(x_test)))\n",
    "    \n",
    "    \n",
    "clf_eval(\n",
    "    clf,\n",
    "    x_test,\n",
    "    y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c1c61-1af0-4b0d-81e9-778a13cca4dd",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e56d6-f6cd-4f61-99fc-582ff14abc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['pred_proba'] = pred_df[ft_cols].apply(lambda x : dict(\n",
    "    zip(clf.classes_, clf.predict_proba(x.values[None])[0])\n",
    "), axis = 1)\n",
    "pred_df['pred'] = pred_df[ft_cols].apply(lambda x : clf.predict(x.values[None])[0], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a38be-e8f2-4c76-a4e6-5ec4b2cd1fd4",
   "metadata": {},
   "source": [
    "## Get Annotated & To Annoate Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e16249-1874-4b3d-a954-6637b8aa822f",
   "metadata": {},
   "source": [
    "Predictions with low predicted probabilities will be annotated manually by the user, while predictions which have high predicted probability will be assumed to be a good prediction generated by the model. This will allow us to increase our labelled data and retrain a new model based on the best performing results of the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15d2e12-b94e-47b9-8546-b6df6a55ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low predictions with threshold <= 0.6\n",
    "low_th= 0.6\n",
    "\n",
    "# high predictions with threshold >= 0.9\n",
    "high_th = 0.9\n",
    "\n",
    "def check_preds(pred_dct, low_th = low_th, high_th = high_th):\n",
    "    '''\n",
    "    This function will check the dictionary associated to the prediction probabilities\n",
    "    generated by the model. It will return either `annotate`, `annotated` or `neither`\n",
    "    as a result.\n",
    "    \n",
    "    params:\n",
    "        pred_dct (Dictionary) : The keys are the classes, values are the predicted proba\n",
    "        low_th (Integer) : The low prediction proba threshold\n",
    "        high_th (Integer): The high prediction proba threshold\n",
    "        \n",
    "    returns:\n",
    "        This function will return the following:\n",
    "            `annotate` : if the maximum value in the pred_dct is less than or equal to\n",
    "                         the low_th\n",
    "            `annotated` : if the maximum value in the pred_dct is greater than or equal to\n",
    "                          the high_th\n",
    "            'neither' : If it does not fall in the other two ranges\n",
    "            \n",
    "    example:\n",
    "        pred_df['annotate_decision'] = pred_df['pred_proba'].apply(check_preds)\n",
    "    '''\n",
    "    \n",
    "    max_val = max(list(pred_dct.values()))\n",
    "    if max_val <= low_th:\n",
    "        return 'annotate'\n",
    "    elif max_val >= high_th:\n",
    "        return 'annotated'\n",
    "    else:\n",
    "        return 'neither'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a780f-cbd5-4e74-877f-fcb4343385f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['annotate_decision'] = pred_df['pred_proba'].apply(check_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba008a-6fd3-4ee8-a76e-2ec29ca8b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['annotate_decision'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34a2e89-fca9-464c-99ab-e38973ab3ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_df = pred_df[pred_df['annotate_decision'] == 'annotated'].copy()\n",
    "annotate_df = pred_df[pred_df['annotate_decision'] == 'annotate'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e596f-2bac-4e45-98ad-e286995a5f91",
   "metadata": {},
   "source": [
    "I'm just going to randomly assign a gender out of simplicity (since this is randomly generated data), but in reality you should manually annotate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e9f577-c67c-4b44-a010-28e9ca3aa38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_choices = ['Male', 'Female', np.nan, np.nan, np.nan]\n",
    "annotate_df['gender'] = [random.choice(gender_choices) for _ in range(annotate_df.shape[0])]\n",
    "annotate_df = annotate_df[~annotate_df['gender'].isna()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3927f9-97ee-4837-89e1-2a0fdeabfb8b",
   "metadata": {},
   "source": [
    "## Update Labelled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be52ed2f-3781-4ade-8420-ad5c62e5d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_df['gender'] = annotated_df['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4bcc6-60ae-4dc5-896a-63f7e371334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_df = pd.concat([annotated_df, annotate_df, train_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc574977-bc23-40c8-a7e0-6af29de96728",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e3d88-0337-4253-860d-928560352953",
   "metadata": {},
   "source": [
    "## Retrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0ddb38-acbb-4c45-adbd-e2db94541d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature, target breakdown\n",
    "ft_cols = ['height', 'weight', 'age']\n",
    "target_col = 'gender'\n",
    "\n",
    "# train test split\n",
    "x = annotated_df[ft_cols].values\n",
    "y = annotated_df[target_col].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, \n",
    "    y,\n",
    "    test_size = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e098dfd-ceb1-468b-8f08-91c95025e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBC classifier\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "# train the model\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c2cf05-744c-495e-a049-daf06bdcdc10",
   "metadata": {},
   "source": [
    "## Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda2526-0db6-48e3-9a6b-0b520925c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_eval(\n",
    "    clf,\n",
    "    x_test,\n",
    "    y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31b3819-66a8-4579-893d-69a10829458a",
   "metadata": {},
   "source": [
    "## Concluding Remarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf0983e-e65b-4402-aaf3-23f3739b8dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "183fc07d-afeb-48bc-9d0d-1eebfba87d05",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
