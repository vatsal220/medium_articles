{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f17e3f-fcce-4aec-93f9-ad76897323cf",
   "metadata": {},
   "source": [
    "# Link Prediction w/ n2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c26a7c0-135c-40c7-84af-16260c96793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77beb5bb-e6db-4d92-88fb-2ee138c24836",
   "metadata": {},
   "source": [
    "You can install the arxiv package in Python with the following command:  \n",
    "`pip install arxiv`  \n",
    "or follow the instructions here : https://pypi.org/project/arxiv/  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53441e5-efb1-4add-9c77-97f08a30f6de",
   "metadata": {},
   "source": [
    "## What is Link Prediction?\n",
    "\n",
    "\n",
    "\n",
    "## Cold Start Problem in Recommendation Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fe6b94e0-f903-422f-baab-76010e772026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import arxiv\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, confusion_matrix, classification_report\n",
    "from itertools import product\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from node2vec import Node2Vec as n2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5116e841-87c8-4e79-9c1f-6b740c7a9ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "queries = [\n",
    "    'automl', 'machinelearning', 'data', 'phyiscs','mathematics', 'recommendation system', 'nlp', 'neural networks'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b445bcd4-ecdd-4b72-84ff-e36afc5e1a2c",
   "metadata": {},
   "source": [
    "# Fetch Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4e4258-9565-41b0-a01a-4ef1cf783c01",
   "metadata": {},
   "source": [
    "We want to hit th Arxiv API to gather some information about the latest research papers based on the queries we've identified above. This will allow us to then create a network from this research paper data and then we can try to predict links on that network. For the purposes of this article, I will search for a maximum of 1000 results per query, but you don't have to set yourself to the same constraints. The Arxiv API allows users to hit up to 300,000 results per query. The function outlined below will generate a CSV fetching the following information :   \n",
    "```'title', 'date', 'article_id', 'url', 'main_topic', 'all_topics', 'authors', 'year'```   \n",
    "You are able to fetch more information like the `links, summary, article` but I decided not to since those features won't really be used for the purposes of this analysis and tutorial.\n",
    "\n",
    "For reference to the Arxiv API, you can find their detailed documentation here : https://arxiv.org/help/api/user-manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14298db4-2590-4992-a49c-d6c93d962b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv(queries, max_results = 1000):\n",
    "    '''\n",
    "    This function will search arxiv associated to a set of queries and store\n",
    "    the latest 10000 (max_results) associated to that search.\n",
    "    \n",
    "    params:\n",
    "        queries (List -> Str) : A list of strings containing keywords you want\n",
    "                                to search on Arxiv\n",
    "        max_results (Int) : The maximum number of results you want to see associated\n",
    "                            to your search. Default value is 1000, capped at 300000\n",
    "                            \n",
    "    returns:\n",
    "        This function will return a DataFrame holding the following columns associated\n",
    "        to the queries the user has passed. \n",
    "            `title`, `date`, `article_id`, `url`, `main_topic`, `all_topics`\n",
    "    \n",
    "    example:\n",
    "        research_df = search_arxiv(\n",
    "            queries = ['automl', 'recommender system', 'nlp', 'data science'],\n",
    "            max_results = 10000\n",
    "        )\n",
    "    '''\n",
    "    d = []\n",
    "    searches = []\n",
    "    # hitting the API\n",
    "    for query in queries:\n",
    "        search = arxiv.Search(\n",
    "          query = query,\n",
    "          max_results = max_results,\n",
    "          sort_by = arxiv.SortCriterion.SubmittedDate,\n",
    "          sort_order = arxiv.SortOrder.Descending\n",
    "        )\n",
    "        searches.append(search)\n",
    "    \n",
    "    # Converting search result into df\n",
    "    for search in searches:\n",
    "        for res in search.results():\n",
    "            data = {\n",
    "                'title' : res.title,\n",
    "                'date' : res.published,\n",
    "                'article_id' : res.entry_id,\n",
    "                'url' : res.pdf_url,\n",
    "                'main_topic' : res.primary_category,\n",
    "                'all_topics' : res.categories,\n",
    "                'authors' : res.authors\n",
    "            }\n",
    "            d.append(data)\n",
    "        \n",
    "    d = pd.DataFrame(d)\n",
    "    d['year'] = pd.DatetimeIndex(d['date']).year\n",
    "    \n",
    "    # change article id from url to integer\n",
    "    unique_article_ids = d.article_id.unique()\n",
    "    article_mapping = {art:idx for idx,art in enumerate(unique_article_ids)}\n",
    "    d['article_id'] = d['article_id'].map(article_mapping)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96f77985-1287-4641-bd45-9dbd873c0da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 987 ms, sys: 46.1 ms, total: 1.03 s\n",
      "Wall time: 7.79 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(646, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "research_df = search_arxiv(\n",
    "    queries = queries,\n",
    "    max_results = 100\n",
    ")\n",
    "research_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e278a-23de-4671-8bd4-853636941495",
   "metadata": {},
   "source": [
    "If you're having trouble querying the data, for reproducibility purposes, the CSV I used for the analysis conducted in this article was uploaded to my GitHub which you can find here. https://github.com/vatsal220/medium_articles/blob/main/link_prediction/data/arxiv_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b32ed-3c68-412a-bf34-798292d47684",
   "metadata": {},
   "source": [
    "## Generate Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b2d6f-60f9-4568-afc2-50cb32154cff",
   "metadata": {},
   "source": [
    "Now that we've fetched the data using the Arxiv API, we can generate a network. The network will have the following structure, nodes will be the article_ids and the edges will be all topics connecting a pair of articles. For example, article_id 1 with the following topics `astro-physics, and stats` can be connected to article_id 10 with the topic `stats` and article_id 7 with the topics `astro-physics, math`. This will be a multi-edge network where each edge will hold a weight of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ef8e9a-0c80-4d4b-a56b-7e50263abacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network(df, node_col = 'article_id', edge_col = 'main_topic'):\n",
    "    '''\n",
    "    This function will generate a article to article network given an input DataFrame.\n",
    "    It will do so by creating an edge_dictionary where each key is going to be a node\n",
    "    referenced by unique values in node_col and the values will be a list of other nodes\n",
    "    connected to the key through the edge_col.\n",
    "    \n",
    "    params:\n",
    "        df (DataFrame) : The dataset which holds the node and edge columns\n",
    "        node_col (String) : The column name associated to the nodes of the network\n",
    "        edge_col (String) : The column name associated to the edges of the network\n",
    "        \n",
    "    returns:\n",
    "        A networkx graph corresponding to the input dataset\n",
    "        \n",
    "    example:\n",
    "        generate_network(\n",
    "            research_df,\n",
    "            node_col = 'article_id',\n",
    "            edge_col = 'main_topic'\n",
    "        )\n",
    "    '''\n",
    "    edge_dct = {}\n",
    "    for i,g in df.groupby(node_col):\n",
    "        topics = g[edge_col].unique()\n",
    "        edge_df = df[(df[node_col] != i) & (df[edge_col].isin(topics))]\n",
    "        edges = list(edge_df[node_col].unique())\n",
    "        edge_dct[i] = edges\n",
    "    \n",
    "    # create nx network\n",
    "    g = nx.Graph(edge_dct, create_using = nx.MultiGraph)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa528e7e-88df-487a-b364-f9b04573d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tp = research_df.explode('all_topics').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b98f8dc1-3bf7-45ce-b2ad-f69c1e08b8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 443 ms, sys: 34.5 ms, total: 477 ms\n",
      "Wall time: 490 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tp_nx = generate_network(\n",
    "    all_tp, \n",
    "    node_col = 'article_id', \n",
    "    edge_col = 'all_topics'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5afa0c52-38c6-4da4-8ac9-358df0aefb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 570\n",
      "Number of edges: 26178\n",
      "Average degree:  91.8526\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(tp_nx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b646d2a-1d5e-4837-870b-7d99153d2728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 351 ms, sys: 3.65 ms, total: 354 ms\n",
      "Wall time: 353 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "research_nx = generate_network(\n",
    "    research_df, \n",
    "    node_col = 'article_id', \n",
    "    edge_col = 'main_topic'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e45a5537-68da-4a97-ae74-b0ccdbcc84be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 570\n",
      "Number of edges: 10698\n",
      "Average degree:  37.5368\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(research_nx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66415397-621f-4ae9-9e2c-388359e33478",
   "metadata": {},
   "source": [
    "## Node2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97bc9ab-f39b-4ef8-833c-6cbe7e1e65ef",
   "metadata": {},
   "source": [
    "This component will cover running node2vec on the graph generated above and creating the associated node embeddings for that network. These embeddings will play a crucial role coming up as they're the main features necessary for building a link prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "066bf93a-ad52-491b-adb1-79fe6481ab9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca960e365f064ff394460ae25c95a8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [00:10<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.7 s, sys: 143 ms, total: 13.9 s\n",
      "Wall time: 13.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time g_emb = n2v(research_nx, dimensions=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afca8a21-5aa8-493a-ba57-e83695637ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 1 # Node2Vec fit window\n",
    "MIN_COUNT = 1 # Node2Vec min. count\n",
    "BATCH_WORDS = 4 # Node2Vec batch words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5ce72cd-db36-430f-b65e-f7b9b9604120",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = g_emb.fit(\n",
    "    window=WINDOW,\n",
    "    min_count=MIN_COUNT,\n",
    "    batch_words=BATCH_WORDS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eda734d7-6b08-4a76-aca9-6a54948c1e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('300', 0.6815548539161682)\n",
      "('395', 0.6645826697349548)\n",
      "('109', 0.6567216515541077)\n",
      "('363', 0.6363925337791443)\n",
      "('204', 0.6336174607276917)\n",
      "('371', 0.6288920640945435)\n",
      "('139', 0.6258803009986877)\n",
      "('458', 0.6248643398284912)\n",
      "('566', 0.6243730187416077)\n",
      "('274', 0.6211979389190674)\n"
     ]
    }
   ],
   "source": [
    "input_node = '1'\n",
    "for s in mdl.wv.most_similar(input_node, topn = 10):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a904f-fc1c-4a60-972c-94f39143f64d",
   "metadata": {},
   "source": [
    "## Generate Embeddings DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08f66aff-fe52-4ba1-85e4-32d9ec1cb5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = (\n",
    "    pd.DataFrame(\n",
    "        [mdl.wv.get_vector(str(n)) for n in research_nx.nodes()],\n",
    "        index = research_nx.nodes\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca946a47-125a-41a5-9a36-bfd1543d6ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.511957</td>\n",
       "      <td>-0.580585</td>\n",
       "      <td>-0.626385</td>\n",
       "      <td>-0.827639</td>\n",
       "      <td>0.904748</td>\n",
       "      <td>0.777848</td>\n",
       "      <td>0.182476</td>\n",
       "      <td>-1.217629</td>\n",
       "      <td>-0.396125</td>\n",
       "      <td>-1.085773</td>\n",
       "      <td>-0.119347</td>\n",
       "      <td>0.081139</td>\n",
       "      <td>-0.253311</td>\n",
       "      <td>0.879596</td>\n",
       "      <td>0.556270</td>\n",
       "      <td>0.284939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.549100</td>\n",
       "      <td>0.134050</td>\n",
       "      <td>-1.983533</td>\n",
       "      <td>-1.282142</td>\n",
       "      <td>0.762598</td>\n",
       "      <td>0.978624</td>\n",
       "      <td>0.186682</td>\n",
       "      <td>0.436884</td>\n",
       "      <td>-1.926421</td>\n",
       "      <td>-1.661931</td>\n",
       "      <td>-0.821661</td>\n",
       "      <td>-2.232859</td>\n",
       "      <td>-1.446645</td>\n",
       "      <td>-0.479345</td>\n",
       "      <td>1.687179</td>\n",
       "      <td>-1.335484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.488350</td>\n",
       "      <td>-0.587227</td>\n",
       "      <td>-0.625464</td>\n",
       "      <td>-0.782213</td>\n",
       "      <td>0.869252</td>\n",
       "      <td>0.836026</td>\n",
       "      <td>0.185684</td>\n",
       "      <td>-1.158214</td>\n",
       "      <td>-0.469872</td>\n",
       "      <td>-1.117828</td>\n",
       "      <td>-0.063733</td>\n",
       "      <td>0.220845</td>\n",
       "      <td>-0.207277</td>\n",
       "      <td>0.864076</td>\n",
       "      <td>0.567868</td>\n",
       "      <td>0.222027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.520829</td>\n",
       "      <td>-0.603254</td>\n",
       "      <td>-0.620381</td>\n",
       "      <td>-0.765303</td>\n",
       "      <td>0.907810</td>\n",
       "      <td>0.850281</td>\n",
       "      <td>0.182577</td>\n",
       "      <td>-1.161945</td>\n",
       "      <td>-0.394557</td>\n",
       "      <td>-1.037542</td>\n",
       "      <td>-0.090025</td>\n",
       "      <td>0.194334</td>\n",
       "      <td>-0.269508</td>\n",
       "      <td>0.866738</td>\n",
       "      <td>0.623747</td>\n",
       "      <td>0.283081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.536191</td>\n",
       "      <td>-0.615628</td>\n",
       "      <td>-0.488054</td>\n",
       "      <td>-0.758666</td>\n",
       "      <td>0.870371</td>\n",
       "      <td>0.816817</td>\n",
       "      <td>0.229364</td>\n",
       "      <td>-1.192694</td>\n",
       "      <td>-0.459576</td>\n",
       "      <td>-1.160274</td>\n",
       "      <td>-0.103331</td>\n",
       "      <td>0.272849</td>\n",
       "      <td>-0.131484</td>\n",
       "      <td>0.901864</td>\n",
       "      <td>0.598280</td>\n",
       "      <td>0.257405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.511957 -0.580585 -0.626385 -0.827639  0.904748  0.777848  0.182476   \n",
       "1 -1.549100  0.134050 -1.983533 -1.282142  0.762598  0.978624  0.186682   \n",
       "2 -0.488350 -0.587227 -0.625464 -0.782213  0.869252  0.836026  0.185684   \n",
       "3 -0.520829 -0.603254 -0.620381 -0.765303  0.907810  0.850281  0.182577   \n",
       "4 -0.536191 -0.615628 -0.488054 -0.758666  0.870371  0.816817  0.229364   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0 -1.217629 -0.396125 -1.085773 -0.119347  0.081139 -0.253311  0.879596   \n",
       "1  0.436884 -1.926421 -1.661931 -0.821661 -2.232859 -1.446645 -0.479345   \n",
       "2 -1.158214 -0.469872 -1.117828 -0.063733  0.220845 -0.207277  0.864076   \n",
       "3 -1.161945 -0.394557 -1.037542 -0.090025  0.194334 -0.269508  0.866738   \n",
       "4 -1.192694 -0.459576 -1.160274 -0.103331  0.272849 -0.131484  0.901864   \n",
       "\n",
       "         14        15  \n",
       "0  0.556270  0.284939  \n",
       "1  1.687179 -1.335484  \n",
       "2  0.567868  0.222027  \n",
       "3  0.623747  0.283081  \n",
       "4  0.598280  0.257405  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd6bb7-c281-47a8-a62c-3970845ef989",
   "metadata": {},
   "source": [
    "## Recommendations w/ Distance Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0017cc-caf6-40b2-ad3c-3aaca18d4994",
   "metadata": {},
   "source": [
    "Now that we have a embedding vector representing each node in the network, we can then use distance measures like cosine similarity, euclidean distance, manhattan distance, etc. to measure the amount of distance between nodes. The assumption that we're making by using these distance measures is that nodes in close proximity with each other should also have an edge connecting each other. This is a good assumption to make as node2vec tries to preserve the initial structure of the original input graph. Now we can essentially write out code to measure similarity levels between two vectors using cosine similarity (or a different distance measure) and identify pairs of nodes which don't currently have an edge between them but do have a large similarity should create an edge between them. This interpretation can be different for multi / weighted / directed graphs. Pick and use a similarity measure appropriate to the network and problem you're trying to solve. Also be aware that different measures have different interperations, for this problem you want to pick maximal cosine similarity scores whereas if you were to use something like euclidean distance, you would want to pick the minimal distance between two vectors.\n",
    "\n",
    "On a side note, I do want to mention that the curse of dimensionality is rampant when solving these types of problems. It's especially problematic when using euclidean distance in particular to measure the distance between vectors in higher dimensions. The term higher dimensions is broad and open to interpretation, the threshold for a dimension to be \"high\" is not strictly defined and varies from problem to problem. Without going to deep into the mathematics behind things, euclidean distance is not a good measure to use for sparse or high dimensional vectors. You can reference this post on stack exchange which outlines the mathematical reasoning as to why this is the case. \n",
    "\n",
    "- https://stats.stackexchange.com/questions/29627/euclidean-distance-is-usually-not-good-for-sparse-data-and-more-general-case\n",
    "- https://stats.stackexchange.com/questions/99171/why-is-euclidean-distance-not-a-good-metric-in-high-dimensions\n",
    "\n",
    "For more on the curse of dimensionality, refer to this [paper](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) by the Computer Science department from the University of Washington."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edf712d2-f670-4d47-b60a-3854fe138bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_links(G, df, article_id, N):\n",
    "    '''\n",
    "    This function will predict the top N links a node (article_id) should be connected with\n",
    "    which it is not already connected with in G.\n",
    "    \n",
    "    params:\n",
    "        G (Netowrkx Graph) : The network used to create the embeddings\n",
    "        df (DataFrame) : The dataframe which has embeddings associated to each node\n",
    "        article_id (Integer) : The article you're interested \n",
    "        N (Integer) : The number of recommended links you want to return\n",
    "        \n",
    "    returns:\n",
    "        This function will return a list of nodes the input node should be connected with.\n",
    "    '''\n",
    "    \n",
    "    # separate target article with all others\n",
    "    article = df[df.index == article_id]\n",
    "    \n",
    "    # other articles are all articles which the current doesn't have an edge connecting\n",
    "    all_nodes = G.nodes()\n",
    "    other_nodes = [n for n in all_nodes if n not in list(G.adj[article_id]) + [article_id]]\n",
    "    other_articles = df[df.index.isin(other_nodes)]\n",
    "    \n",
    "    # get similarity of current reader and all other readers\n",
    "    sim = cosine_similarity(article, other_articles)[0].tolist()\n",
    "    idx = other_articles.index.tolist()\n",
    "    \n",
    "    # create a similarity dictionary for this user w.r.t all other users\n",
    "    idx_sim = dict(zip(idx, sim))\n",
    "    idx_sim = sorted(idx_sim.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    similar_articles = idx_sim[:N]\n",
    "    articles = [art[0] for art in similar_articles]\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d620ab80-41f1-438c-9f10-f9a5bdcaf590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[300, 395, 109, 363, 204, 371, 139, 458, 566, 274]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_links(G = research_nx, df = emb_df, article_id = 1, N = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab008511-b524-4207-9177-f4cca968fdc6",
   "metadata": {},
   "source": [
    "## Modelling Based Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61708a7e-18c4-4cb3-b6e8-74bc2446b13d",
   "metadata": {},
   "source": [
    "In this section we're going to build a binary classifier to predict the probability of a pair of edges to be connected or not. To do this we first need to identify all pairs of nodes which can form an edge, and identify the subset of those pairs which have already have an edge between them in the original network. We can then combine the embeddings (through vector addition) associated to all possible edges and pass that into a train test split function to generate training and testing partitions to pass into a classification model. \n",
    "\n",
    "The model I'll be using for this tutorial will be a gradient boosting classifier, for your own problems & experiments I advise you to try out a variety of different classifiers and selected the overall best performing one. We are building a binary classifier, however in almost all situations regarldess of the input data / network there will be a class imbalance on this classifier. Since we're taking all possible cominbations of edges which can be formed in the network, that result is N^N (where N is the number of nodes in the network), this is exponentially large. The only way that this wouldn't be a binary classifier is if your graph is already almost fully connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8eb0724-c0fa-4124-966e-587ef2a0e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_nodes = list(research_nx.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9994925c-c38d-4681-af7d-880398bdb0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.4 ms, sys: 11.8 ms, total: 54.3 ms\n",
      "Wall time: 57.6 ms\n"
     ]
    }
   ],
   "source": [
    "%time all_possible_edges = [(x,y) for (x,y) in product(unique_nodes, unique_nodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eca2b97c-1bf9-4b56-84ee-8cef35670442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162165"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_possible_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e275d29-759e-44d6-bdd2-7c18435f893e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324900"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_possible_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "24f60661-7621-45b4-972c-8f571744a9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 734 ms, sys: 32.4 ms, total: 766 ms\n",
      "Wall time: 780 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "edge_features = [\n",
    "    (mdl.wv.get_vector(str(i)) + mdl.wv.get_vector(str(j))) for i,j in all_possible_edges\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "323cde89-2c53-4ccb-8365-064624819fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = list(research_nx.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2bdf3789-8348-4c8b-86e9-5e8fa8f09797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 4s, sys: 227 ms, total: 1min 4s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%time is_con = [1 if e in edges else 0 for e in all_possible_edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fa3a543-06db-4a37-886e-f1e3cb6faf64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10698"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(is_con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fe205e-cc61-4f91-acce-449792808bb0",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d16a8551-a343-4807-bab1-393e1293fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(edge_features)\n",
    "y = is_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "874593a1-0f94-44b4-b758-485171ed0af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2309d219-ecc9-46b0-9fcb-03d9a7afb64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 57s, sys: 351 ms, total: 1min 57s\n",
      "Wall time: 1min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# classifier\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "# train the model\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d1153-1e48-499e-9dc0-122c3425a705",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ea4a6-2700-492b-9916-2afd5161b4b8",
   "metadata": {},
   "source": [
    "When dealing with class imbalances we can't look at traditional accuracy measures to evaluate the performance of the model. This is because if the model continously predicts the class which has the greater amount of data, then regardless of the model never predicting the other class, it will still achieve a high level of accuracy. The way to combat this is to use measures like Matthews Correlation Coefficient. \n",
    "\n",
    "```quote\n",
    "The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient.\n",
    "```\n",
    "- https://en.wikipedia.org/wiki/Phi_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0022514-47e6-4f97-919d-c951fde21abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test)\n",
    "y_true = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "70f235e0-9c64-4474-90c6-ccd11d781363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy :  0.9658356417359187\n",
      "Training Accuracy :  0.9673704729660408\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(x_test)\n",
    "x_pred = clf.predict(x_train)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "train_acc = accuracy_score(y_train, x_pred)\n",
    "print(\"Testing Accuracy : \", test_acc)\n",
    "print(\"Training Accuracy : \", train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af2e8de6-4636-4443-95e8-d7e4c0c1691c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.059086223731077636"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_corrcoef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "67a09428-6855-4737-8041-4caead755055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[78408,  2667],\n",
       "       [  108,    42]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9d2a460b-e039-45a9-ac25-411dae6181ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98     94216\n",
      "           1       0.02      0.00      0.00      3254\n",
      "\n",
      "    accuracy                           0.96     97470\n",
      "   macro avg       0.49      0.50      0.49     97470\n",
      "weighted avg       0.93      0.96      0.95     97470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Classification Report : \")\n",
    "print(classification_report(y_test, clf.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463d29d4-49ad-43c9-b394-cc3088009dbe",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3d929e1-d590-4ed7-b835-b21e60d9008c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ft = [(mdl.wv.get_vector(str('42'))+mdl.wv.get_vector(str('210')))]\n",
    "clf.predict(pred_ft)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5146010-f35c-4f23-a8e4-5882cb0cbce3",
   "metadata": {},
   "source": [
    "Clearly as you can see that this is a poor performing model for it's given task (reflected by the MCC, ft-score, recall and percision). But that's alright since this article was simply for an educational purpose. Not every approach you use to solve a given problem will pan out or work, often times its not at fault of the approach but rather the data. In the case of this tutorial, the data was definetly at fault since I was only using a small sample of a actual research network which would be available if I scraped Arxiv for more data. But doing that would also increase the computational complexity to a lot of different components of this article (like running node2vec, generating all possible pairs of edges, training the model, etc.). \n",
    "\n",
    "The rule of thumb most data scientists follow when solving problems is that if the simple solutions work, then the more complex solutions should also work. Simpler solutions in recommendation systems involving collaborative filtering / content based are often quite easy to implement and yield relatively informative results and indications whether or not a more complex solution would work for solving this problem as well. Its often not the best to throw a neural network at the problem (like node2vec) for a variety of reasons (like training time, interpretability, model inference time, etc.). \n",
    "\n",
    "For a guideline on how to implement the simpler solutions in recommendation systems, you can refer to another article I've written [here](). Furthermore, if you want to follow through in the jupyter notebook associated to this project, you can reference my [GitHub]()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b67e10-927d-4fc7-bc3b-0ef3115f7c19",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
