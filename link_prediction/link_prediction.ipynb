{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f17e3f-fcce-4aec-93f9-ad76897323cf",
   "metadata": {},
   "source": [
    "# Link Prediction w/ n2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c26a7c0-135c-40c7-84af-16260c96793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77beb5bb-e6db-4d92-88fb-2ee138c24836",
   "metadata": {},
   "source": [
    "You can install the arxiv package in Python with the following command:  \n",
    "`pip install arxiv`  \n",
    "or follow the instructions here : https://pypi.org/project/arxiv/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6b94e0-f903-422f-baab-76010e772026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import arxiv\n",
    "\n",
    "from node2vec import node2vec as n2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5116e841-87c8-4e79-9c1f-6b740c7a9ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "queries = [\n",
    "    'automl', 'machinelearning', 'data', 'phyiscs','mathematics', 'recommendation system', 'nlp', 'neural networks'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b445bcd4-ecdd-4b72-84ff-e36afc5e1a2c",
   "metadata": {},
   "source": [
    "# Fetch Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4e4258-9565-41b0-a01a-4ef1cf783c01",
   "metadata": {},
   "source": [
    "We want to hit th Arxiv API to gather some information about the latest research papers based on the queries we've identified above. This will allow us to then create a network from this research paper data and then we can try to predict links on that network. For the purposes of this article, I will search for a maximum of 1000 results per query, but you don't have to set yourself to the same constraints. The Arxiv API allows users to hit up to 300,000 results per query. The function outlined below will generate a CSV fetching the following information :   \n",
    "```'title', 'date', 'article_id', 'url', 'main_topic', 'all_topics', 'authors', 'year'```   \n",
    "You are able to fetch more information like the `links, summary, article` but I decided not to since those features won't really be used for the purposes of this analysis and tutorial.\n",
    "\n",
    "For reference to the Arxiv API, you can find their detailed documentation here : https://arxiv.org/help/api/user-manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14298db4-2590-4992-a49c-d6c93d962b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv(queries, max_results = 1000):\n",
    "    '''\n",
    "    This function will search arxiv associated to a set of queries and store\n",
    "    the latest 10000 (max_results) associated to that search.\n",
    "    \n",
    "    params:\n",
    "        queries (List -> Str) : A list of strings containing keywords you want\n",
    "                                to search on Arxiv\n",
    "        max_results (Int) : The maximum number of results you want to see associated\n",
    "                            to your search. Default value is 1000, capped at 300000\n",
    "                            \n",
    "    returns:\n",
    "        This function will return a DataFrame holding the following columns associated\n",
    "        to the queries the user has passed. \n",
    "            `title`, `date`, `article_id`, `url`, `main_topic`, `all_topics`\n",
    "    \n",
    "    example:\n",
    "        research_df = search_arxiv(\n",
    "            queries = ['automl', 'recommender system', 'nlp', 'data science'],\n",
    "            max_results = 10000\n",
    "        )\n",
    "    '''\n",
    "    d = []\n",
    "    searches = []\n",
    "    # hitting the API\n",
    "    for query in queries:\n",
    "        search = arxiv.Search(\n",
    "          query = query,\n",
    "          max_results = max_results,\n",
    "          sort_by = arxiv.SortCriterion.SubmittedDate,\n",
    "          sort_order = arxiv.SortOrder.Descending\n",
    "        )\n",
    "        searches.append(search)\n",
    "    \n",
    "    # Converting search result into df\n",
    "    for search in searches:\n",
    "        for res in search.results():\n",
    "            data = {\n",
    "                'title' : res.title,\n",
    "                'date' : res.published,\n",
    "                'article_id' : res.entry_id,\n",
    "                'url' : res.pdf_url,\n",
    "                'main_topic' : res.primary_category,\n",
    "                'all_topics' : res.categories,\n",
    "                'authors' : res.authors\n",
    "            }\n",
    "            d.append(data)\n",
    "        \n",
    "    d = pd.DataFrame(d)\n",
    "    d['year'] = pd.DatetimeIndex(d['date']).year\n",
    "    \n",
    "    # change article id from url to integer\n",
    "    unique_article_ids = d.article_id.unique()\n",
    "    article_mapping = {art:idx for idx,art in enumerate(unique_article_ids)}\n",
    "    d['article_id'] = d['article_id'].map(article_mapping)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96f77985-1287-4641-bd45-9dbd873c0da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.02 s, sys: 456 ms, total: 8.48 s\n",
      "Wall time: 3min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5332, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "research_df = search_arxiv(\n",
    "    queries = queries,\n",
    "    max_results = 1000\n",
    ")\n",
    "research_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e278a-23de-4671-8bd4-853636941495",
   "metadata": {},
   "source": [
    "If you're having trouble querying the data, for reproducibility purposes, the CSV I used for the analysis conducted in this article was uploaded to my GitHub which you can find here. https://github.com/vatsal220/medium_articles/blob/main/link_prediction/data/arxiv_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b32ed-3c68-412a-bf34-798292d47684",
   "metadata": {},
   "source": [
    "## Generate Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b2d6f-60f9-4568-afc2-50cb32154cff",
   "metadata": {},
   "source": [
    "Now that we've fetched the data using the Arxiv API, we can generate a network. The network will have the following structure, nodes will be the article_ids and the edges will be all topics connecting a pair of articles. For example, article_id 1 with the following topics `astro-physics, and stats` can be connected to article_id 10 with the topic `stats` and article_id 7 with the topics `astro-physics, math`. This will be a multi-edge network where each edge will hold a weight of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec9d6b08-ef13-4ac1-bc69-61529d33679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_df = research_df.explode('all_topics').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab8e97-3700-4d7d-b1ef-13a94584d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network(df):\n",
    "    '''\n",
    "    This function will generate a article to article network given an input DataFrame.\n",
    "    '''\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfa77b56-7a2f-4f98-96e9-561c7b9935ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.76 s, sys: 428 ms, total: 5.19 s\n",
      "Wall time: 5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "edge_dct = {}\n",
    "for i,g in r_df.groupby('article_id'):\n",
    "    top = g.all_topics.unique()\n",
    "    edge_df = r_df[(r_df.article_id != i) & (r_df.all_topics.isin(top))]\n",
    "    connection_nodes = list(edge_df.article_id.unique())\n",
    "    edge_dct[i] = connection_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f8789c2-b4d1-4a89-8348-28870608cc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.06 s, sys: 297 ms, total: 4.36 s\n",
      "Wall time: 4.48 s\n"
     ]
    }
   ],
   "source": [
    "%time g = nx.Graph(edge_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d118bca1-e620-4348-8d76-16f53ee8265c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 4514\n",
      "Number of edges: 1580377\n",
      "Average degree: 700.2113\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "053e56d3-1f74-4b22-ba27-f972aa1cc0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4514"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_df.article_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16641c8-527f-4836-91eb-64fd20312914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
