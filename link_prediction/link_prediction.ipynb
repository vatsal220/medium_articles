{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f17e3f-fcce-4aec-93f9-ad76897323cf",
   "metadata": {},
   "source": [
    "# Link Prediction w/ n2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c26a7c0-135c-40c7-84af-16260c96793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77beb5bb-e6db-4d92-88fb-2ee138c24836",
   "metadata": {},
   "source": [
    "You can install the arxiv package in Python with the following command:  \n",
    "`pip install arxiv`  \n",
    "or follow the instructions here : https://pypi.org/project/arxiv/  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53441e5-efb1-4add-9c77-97f08a30f6de",
   "metadata": {},
   "source": [
    "## What is Link Prediction?\n",
    "There are many ways to solve problems in recommendation engines. These solutions range from algorithmic approaches, link prediction algorithms, embedding based solutions, etc. Link prediction is also referred to as graph completion, a common problem in graph theory. In the simplest form, given a network, you want to you want to know if there should be an edge between a pair of nodes. This definition changes slightly depending on the type of network you're working with. A directed / multi graph can have slightly different interpretations but the fundamental concept of identifying missing edges in a network remains.  \n",
    "\n",
    "Problems in link prediction are also quite common when dealing with temporal networks (networks which change over time). Given a network G at time step t, you would want to predict the edges of the graph G at time step t+1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6b94e0-f903-422f-baab-76010e772026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import arxiv\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, confusion_matrix, classification_report\n",
    "from itertools import product\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from node2vec import Node2Vec as n2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5116e841-87c8-4e79-9c1f-6b740c7a9ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "queries = [\n",
    "    'automl', 'machinelearning', 'data', 'phyiscs','mathematics', 'recommendation system', 'nlp', 'neural networks'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b445bcd4-ecdd-4b72-84ff-e36afc5e1a2c",
   "metadata": {},
   "source": [
    "# Fetch Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4e4258-9565-41b0-a01a-4ef1cf783c01",
   "metadata": {},
   "source": [
    "We want to hit th Arxiv API to gather some information about the latest research papers based on the queries we've identified above. This will allow us to then create a network from this research paper data and then we can try to predict links on that network. For the purposes of this article, I will search for a maximum of 1000 results per query, but you don't have to set yourself to the same constraints. The Arxiv API allows users to hit up to 300,000 results per query. The function outlined below will generate a CSV fetching the following information :   \n",
    "```'title', 'date', 'article_id', 'url', 'main_topic', 'all_topics', 'authors', 'year'```   \n",
    "You are able to fetch more information like the `links, summary, article` but I decided not to since those features won't really be used for the purposes of this analysis and tutorial.\n",
    "\n",
    "For reference to the Arxiv API, you can find their detailed documentation here : https://arxiv.org/help/api/user-manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14298db4-2590-4992-a49c-d6c93d962b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv(queries, max_results = 100):\n",
    "    '''\n",
    "    This function will search arxiv associated to a set of queries and store\n",
    "    the latest 10000 (max_results) associated to that search.\n",
    "    \n",
    "    params:\n",
    "        queries (List -> Str) : A list of strings containing keywords you want\n",
    "                                to search on Arxiv\n",
    "        max_results (Int) : The maximum number of results you want to see associated\n",
    "                            to your search. Default value is 1000, capped at 300000\n",
    "                            \n",
    "    returns:\n",
    "        This function will return a DataFrame holding the following columns associated\n",
    "        to the queries the user has passed. \n",
    "            `title`, `date`, `article_id`, `url`, `main_topic`, `all_topics`\n",
    "    \n",
    "    example:\n",
    "        research_df = search_arxiv(\n",
    "            queries = ['automl', 'recommender system', 'nlp', 'data science'],\n",
    "            max_results = 10000\n",
    "        )\n",
    "    '''\n",
    "    d = []\n",
    "    searches = []\n",
    "    # hitting the API\n",
    "    for query in queries:\n",
    "        search = arxiv.Search(\n",
    "          query = query,\n",
    "          max_results = max_results,\n",
    "          sort_by = arxiv.SortCriterion.SubmittedDate,\n",
    "          sort_order = arxiv.SortOrder.Descending\n",
    "        )\n",
    "        searches.append(search)\n",
    "    \n",
    "    # Converting search result into df\n",
    "    for search in searches:\n",
    "        for res in search.results():\n",
    "            data = {\n",
    "                'title' : res.title,\n",
    "                'date' : res.published,\n",
    "                'article_id' : res.entry_id,\n",
    "                'url' : res.pdf_url,\n",
    "                'main_topic' : res.primary_category,\n",
    "                'all_topics' : res.categories,\n",
    "                'authors' : res.authors\n",
    "            }\n",
    "            d.append(data)\n",
    "        \n",
    "    d = pd.DataFrame(d)\n",
    "    d['year'] = pd.DatetimeIndex(d['date']).year\n",
    "    \n",
    "    # change article id from url to integer\n",
    "    unique_article_ids = d.article_id.unique()\n",
    "    article_mapping = {art:idx for idx,art in enumerate(unique_article_ids)}\n",
    "    d['article_id'] = d['article_id'].map(article_mapping)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96f77985-1287-4641-bd45-9dbd873c0da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 964 ms, sys: 69.7 ms, total: 1.03 s\n",
      "Wall time: 8.31 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(646, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "research_df = search_arxiv(\n",
    "    queries = queries,\n",
    "    max_results = 100\n",
    ")\n",
    "research_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "827e67ab-6548-4da5-843b-a3fc0c1bf8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>article_id</th>\n",
       "      <th>url</th>\n",
       "      <th>main_topic</th>\n",
       "      <th>all_topics</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Review of automated time series forecasting pi...</td>\n",
       "      <td>2022-02-03 17:26:27+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>http://arxiv.org/pdf/2202.01712v1</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>[cs.LG]</td>\n",
       "      <td>[Stefan Meisenbacher, Marian Turowski, Kaleb P...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hubble Asteroid Hunter: I. Identifying asteroi...</td>\n",
       "      <td>2022-02-01 06:56:20+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>http://arxiv.org/pdf/2202.00246v1</td>\n",
       "      <td>astro-ph.EP</td>\n",
       "      <td>[astro-ph.EP, astro-ph.IM]</td>\n",
       "      <td>[Sandor Kruk, Pablo García Martín, Marcel Pope...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NAS-Bench-Suite: NAS Evaluation is (Now) Surpr...</td>\n",
       "      <td>2022-01-31 18:02:09+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>http://arxiv.org/pdf/2201.13396v2</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>[cs.LG, cs.AI, stat.ML]</td>\n",
       "      <td>[Yash Mehta, Colin White, Arber Zela, Arjun Kr...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Online AutoML: An adaptive AutoML framework fo...</td>\n",
       "      <td>2022-01-24 15:37:20+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>http://arxiv.org/pdf/2201.09750v1</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>[cs.LG, cs.AI]</td>\n",
       "      <td>[Bilge Celik, Prabhant Singh, Joaquin Vanschoren]</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Automated Reinforcement Learning (AutoRL): A S...</td>\n",
       "      <td>2022-01-11 12:41:43+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>http://arxiv.org/pdf/2201.03916v1</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>[cs.LG]</td>\n",
       "      <td>[Jack Parker-Holder, Raghu Rajan, Xingyou Song...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Review of automated time series forecasting pi...   \n",
       "1  Hubble Asteroid Hunter: I. Identifying asteroi...   \n",
       "2  NAS-Bench-Suite: NAS Evaluation is (Now) Surpr...   \n",
       "3  Online AutoML: An adaptive AutoML framework fo...   \n",
       "4  Automated Reinforcement Learning (AutoRL): A S...   \n",
       "\n",
       "                       date  article_id                                url  \\\n",
       "0 2022-02-03 17:26:27+00:00           0  http://arxiv.org/pdf/2202.01712v1   \n",
       "1 2022-02-01 06:56:20+00:00           1  http://arxiv.org/pdf/2202.00246v1   \n",
       "2 2022-01-31 18:02:09+00:00           2  http://arxiv.org/pdf/2201.13396v2   \n",
       "3 2022-01-24 15:37:20+00:00           3  http://arxiv.org/pdf/2201.09750v1   \n",
       "4 2022-01-11 12:41:43+00:00           4  http://arxiv.org/pdf/2201.03916v1   \n",
       "\n",
       "    main_topic                  all_topics  \\\n",
       "0        cs.LG                     [cs.LG]   \n",
       "1  astro-ph.EP  [astro-ph.EP, astro-ph.IM]   \n",
       "2        cs.LG     [cs.LG, cs.AI, stat.ML]   \n",
       "3        cs.LG              [cs.LG, cs.AI]   \n",
       "4        cs.LG                     [cs.LG]   \n",
       "\n",
       "                                             authors  year  \n",
       "0  [Stefan Meisenbacher, Marian Turowski, Kaleb P...  2022  \n",
       "1  [Sandor Kruk, Pablo García Martín, Marcel Pope...  2022  \n",
       "2  [Yash Mehta, Colin White, Arber Zela, Arjun Kr...  2022  \n",
       "3  [Bilge Celik, Prabhant Singh, Joaquin Vanschoren]  2022  \n",
       "4  [Jack Parker-Holder, Raghu Rajan, Xingyou Song...  2022  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e278a-23de-4671-8bd4-853636941495",
   "metadata": {},
   "source": [
    "If you're having trouble querying the data, for reproducibility purposes, the CSV I used for the analysis conducted in this article was uploaded to my GitHub which you can find here. https://github.com/vatsal220/medium_articles/blob/main/link_prediction/data/arxiv_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b32ed-3c68-412a-bf34-798292d47684",
   "metadata": {},
   "source": [
    "## Generate Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b2d6f-60f9-4568-afc2-50cb32154cff",
   "metadata": {},
   "source": [
    "Now that we've fetched the data using the Arxiv API, we can generate a network. The network will have the following structure, nodes will be the article_ids and the edges will be all topics connecting a pair of articles. For example, article_id 1 with the following topics `astro-physics, and stats` can be connected to article_id 10 with the topic `stats` and article_id 7 with the topics `astro-physics, math`. This will be a multi-edge network where each edge will hold a weight of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ef8e9a-0c80-4d4b-a56b-7e50263abacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network(df, node_col = 'article_id', edge_col = 'main_topic'):\n",
    "    '''\n",
    "    This function will generate a article to article network given an input DataFrame.\n",
    "    It will do so by creating an edge_dictionary where each key is going to be a node\n",
    "    referenced by unique values in node_col and the values will be a list of other nodes\n",
    "    connected to the key through the edge_col.\n",
    "    \n",
    "    params:\n",
    "        df (DataFrame) : The dataset which holds the node and edge columns\n",
    "        node_col (String) : The column name associated to the nodes of the network\n",
    "        edge_col (String) : The column name associated to the edges of the network\n",
    "        \n",
    "    returns:\n",
    "        A networkx graph corresponding to the input dataset\n",
    "        \n",
    "    example:\n",
    "        generate_network(\n",
    "            research_df,\n",
    "            node_col = 'article_id',\n",
    "            edge_col = 'main_topic'\n",
    "        )\n",
    "    '''\n",
    "    edge_dct = {}\n",
    "    for i,g in df.groupby(node_col):\n",
    "        topics = g[edge_col].unique()\n",
    "        edge_df = df[(df[node_col] != i) & (df[edge_col].isin(topics))]\n",
    "        edges = list(edge_df[node_col].unique())\n",
    "        edge_dct[i] = edges\n",
    "    \n",
    "    # create nx network\n",
    "    g = nx.Graph(edge_dct, create_using = nx.MultiGraph)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa528e7e-88df-487a-b364-f9b04573d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tp = research_df.explode('all_topics').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b98f8dc1-3bf7-45ce-b2ad-f69c1e08b8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 418 ms, sys: 31.2 ms, total: 450 ms\n",
      "Wall time: 487 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tp_nx = generate_network(\n",
    "    all_tp, \n",
    "    node_col = 'article_id', \n",
    "    edge_col = 'all_topics'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5afa0c52-38c6-4da4-8ac9-358df0aefb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 570\n",
      "Number of edges: 26178\n",
      "Average degree:  91.8526\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(tp_nx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b646d2a-1d5e-4837-870b-7d99153d2728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 314 ms, sys: 4.24 ms, total: 318 ms\n",
      "Wall time: 317 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "research_nx = generate_network(\n",
    "    research_df, \n",
    "    node_col = 'article_id', \n",
    "    edge_col = 'main_topic'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e45a5537-68da-4a97-ae74-b0ccdbcc84be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 570\n",
      "Number of edges: 10698\n",
      "Average degree:  37.5368\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(research_nx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66415397-621f-4ae9-9e2c-388359e33478",
   "metadata": {},
   "source": [
    "## Node2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97bc9ab-f39b-4ef8-833c-6cbe7e1e65ef",
   "metadata": {},
   "source": [
    "This component will cover running node2vec on the graph generated above and creating the associated node embeddings for that network. These embeddings will play a crucial role coming up as they're the main features necessary for building a link prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "066bf93a-ad52-491b-adb1-79fe6481ab9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7835bf51847e4beabcbbbf0f1c58ef51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [00:13<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.2 s, sys: 356 ms, total: 31.6 s\n",
      "Wall time: 31.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time g_emb = n2v(tp_nx, dimensions=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afca8a21-5aa8-493a-ba57-e83695637ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 1 # Node2Vec fit window\n",
    "MIN_COUNT = 1 # Node2Vec min. count\n",
    "BATCH_WORDS = 4 # Node2Vec batch words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5ce72cd-db36-430f-b65e-f7b9b9604120",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = g_emb.fit(\n",
    "    window=WINDOW,\n",
    "    min_count=MIN_COUNT,\n",
    "    batch_words=BATCH_WORDS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eda734d7-6b08-4a76-aca9-6a54948c1e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('300', 0.9328170418739319)\n",
      "('74', 0.9298872947692871)\n",
      "('133', 0.911675214767456)\n",
      "('158', 0.9070002436637878)\n",
      "('369', 0.8940468430519104)\n",
      "('542', 0.8908052444458008)\n",
      "('194', 0.8712085485458374)\n",
      "('164', 0.8598673343658447)\n",
      "('108', 0.8511670827865601)\n",
      "('159', 0.8436957001686096)\n"
     ]
    }
   ],
   "source": [
    "input_node = '1'\n",
    "for s in mdl.wv.most_similar(input_node, topn = 10):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a904f-fc1c-4a60-972c-94f39143f64d",
   "metadata": {},
   "source": [
    "## Generate Embeddings DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08f66aff-fe52-4ba1-85e4-32d9ec1cb5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = (\n",
    "    pd.DataFrame(\n",
    "        [mdl.wv.get_vector(str(n)) for n in tp_nx.nodes()],\n",
    "        index = tp_nx.nodes\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca946a47-125a-41a5-9a36-bfd1543d6ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.132983</td>\n",
       "      <td>-0.204650</td>\n",
       "      <td>0.031887</td>\n",
       "      <td>0.177110</td>\n",
       "      <td>0.252707</td>\n",
       "      <td>0.924003</td>\n",
       "      <td>1.549395</td>\n",
       "      <td>0.835286</td>\n",
       "      <td>-0.278214</td>\n",
       "      <td>0.152796</td>\n",
       "      <td>0.163435</td>\n",
       "      <td>-0.613121</td>\n",
       "      <td>0.513158</td>\n",
       "      <td>-0.051896</td>\n",
       "      <td>0.260002</td>\n",
       "      <td>0.065459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.260634</td>\n",
       "      <td>-0.260476</td>\n",
       "      <td>0.229207</td>\n",
       "      <td>1.728087</td>\n",
       "      <td>2.118810</td>\n",
       "      <td>-0.464693</td>\n",
       "      <td>0.925524</td>\n",
       "      <td>0.319562</td>\n",
       "      <td>-0.552531</td>\n",
       "      <td>1.545715</td>\n",
       "      <td>-2.290067</td>\n",
       "      <td>0.299350</td>\n",
       "      <td>-0.194123</td>\n",
       "      <td>-0.734371</td>\n",
       "      <td>0.432916</td>\n",
       "      <td>0.520590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.406400</td>\n",
       "      <td>0.532548</td>\n",
       "      <td>-0.137017</td>\n",
       "      <td>-0.176124</td>\n",
       "      <td>0.527928</td>\n",
       "      <td>0.211003</td>\n",
       "      <td>1.947067</td>\n",
       "      <td>0.465411</td>\n",
       "      <td>0.374295</td>\n",
       "      <td>0.086504</td>\n",
       "      <td>0.104123</td>\n",
       "      <td>-0.381165</td>\n",
       "      <td>-0.028701</td>\n",
       "      <td>-0.453330</td>\n",
       "      <td>-0.565627</td>\n",
       "      <td>0.218446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.356758</td>\n",
       "      <td>0.391863</td>\n",
       "      <td>0.175534</td>\n",
       "      <td>0.022510</td>\n",
       "      <td>0.086877</td>\n",
       "      <td>0.517149</td>\n",
       "      <td>1.893181</td>\n",
       "      <td>0.597976</td>\n",
       "      <td>0.423572</td>\n",
       "      <td>0.126311</td>\n",
       "      <td>0.195203</td>\n",
       "      <td>-0.534256</td>\n",
       "      <td>-0.285237</td>\n",
       "      <td>-0.689975</td>\n",
       "      <td>-0.663933</td>\n",
       "      <td>0.352074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.084984</td>\n",
       "      <td>0.008026</td>\n",
       "      <td>0.194727</td>\n",
       "      <td>0.261417</td>\n",
       "      <td>0.289685</td>\n",
       "      <td>1.005614</td>\n",
       "      <td>1.474953</td>\n",
       "      <td>0.789516</td>\n",
       "      <td>-0.154731</td>\n",
       "      <td>0.084582</td>\n",
       "      <td>0.091172</td>\n",
       "      <td>-0.529402</td>\n",
       "      <td>0.513553</td>\n",
       "      <td>-0.139117</td>\n",
       "      <td>0.185542</td>\n",
       "      <td>0.046089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.132983 -0.204650  0.031887  0.177110  0.252707  0.924003  1.549395   \n",
       "1 -0.260634 -0.260476  0.229207  1.728087  2.118810 -0.464693  0.925524   \n",
       "2 -0.406400  0.532548 -0.137017 -0.176124  0.527928  0.211003  1.947067   \n",
       "3 -0.356758  0.391863  0.175534  0.022510  0.086877  0.517149  1.893181   \n",
       "4 -0.084984  0.008026  0.194727  0.261417  0.289685  1.005614  1.474953   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0  0.835286 -0.278214  0.152796  0.163435 -0.613121  0.513158 -0.051896   \n",
       "1  0.319562 -0.552531  1.545715 -2.290067  0.299350 -0.194123 -0.734371   \n",
       "2  0.465411  0.374295  0.086504  0.104123 -0.381165 -0.028701 -0.453330   \n",
       "3  0.597976  0.423572  0.126311  0.195203 -0.534256 -0.285237 -0.689975   \n",
       "4  0.789516 -0.154731  0.084582  0.091172 -0.529402  0.513553 -0.139117   \n",
       "\n",
       "         14        15  \n",
       "0  0.260002  0.065459  \n",
       "1  0.432916  0.520590  \n",
       "2 -0.565627  0.218446  \n",
       "3 -0.663933  0.352074  \n",
       "4  0.185542  0.046089  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd6bb7-c281-47a8-a62c-3970845ef989",
   "metadata": {},
   "source": [
    "## Recommendations w/ Distance Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0017cc-caf6-40b2-ad3c-3aaca18d4994",
   "metadata": {},
   "source": [
    "Now that we have a embedding vector representing each node in the network, we can then use distance measures like cosine similarity, euclidean distance, manhattan distance, etc. to measure the amount of distance between nodes. The assumption that we're making by using these distance measures is that nodes in close proximity with each other should also have an edge connecting each other. This is a good assumption to make as node2vec tries to preserve the initial structure of the original input graph. Now we can essentially write out code to measure similarity levels between two vectors using cosine similarity (or a different distance measure) and identify pairs of nodes which don't currently have an edge between them but do have a large similarity should create an edge between them. This interpretation can be different for multi / weighted / directed graphs. Pick and use a similarity measure appropriate to the network and problem you're trying to solve. Also be aware that different measures have different interperations, for this problem you want to pick maximal cosine similarity scores whereas if you were to use something like euclidean distance, you would want to pick the minimal distance between two vectors.\n",
    "\n",
    "On a side note, I do want to mention that the curse of dimensionality is rampant when solving these types of problems. It's especially problematic when using euclidean distance in particular to measure the distance between vectors in higher dimensions. The term higher dimensions is broad and open to interpretation, the threshold for a dimension to be \"high\" is not strictly defined and varies from problem to problem. Without going to deep into the mathematics behind things, euclidean distance is not a good measure to use for sparse or high dimensional vectors. You can reference this post on stack exchange which outlines the mathematical reasoning as to why this is the case. \n",
    "\n",
    "- https://stats.stackexchange.com/questions/29627/euclidean-distance-is-usually-not-good-for-sparse-data-and-more-general-case\n",
    "- https://stats.stackexchange.com/questions/99171/why-is-euclidean-distance-not-a-good-metric-in-high-dimensions\n",
    "\n",
    "For more on the curse of dimensionality, refer to this [paper](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) by the Computer Science department from the University of Washington."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edf712d2-f670-4d47-b60a-3854fe138bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_links(G, df, article_id, N):\n",
    "    '''\n",
    "    This function will predict the top N links a node (article_id) should be connected with\n",
    "    which it is not already connected with in G.\n",
    "    \n",
    "    params:\n",
    "        G (Netowrkx Graph) : The network used to create the embeddings\n",
    "        df (DataFrame) : The dataframe which has embeddings associated to each node\n",
    "        article_id (Integer) : The article you're interested \n",
    "        N (Integer) : The number of recommended links you want to return\n",
    "        \n",
    "    returns:\n",
    "        This function will return a list of nodes the input node should be connected with.\n",
    "    '''\n",
    "    \n",
    "    # separate target article with all others\n",
    "    article = df[df.index == article_id]\n",
    "    \n",
    "    # other articles are all articles which the current doesn't have an edge connecting\n",
    "    all_nodes = G.nodes()\n",
    "    other_nodes = [n for n in all_nodes if n not in list(G.adj[article_id]) + [article_id]]\n",
    "    other_articles = df[df.index.isin(other_nodes)]\n",
    "    \n",
    "    # get similarity of current reader and all other readers\n",
    "    sim = cosine_similarity(article, other_articles)[0].tolist()\n",
    "    idx = other_articles.index.tolist()\n",
    "    \n",
    "    # create a similarity dictionary for this user w.r.t all other users\n",
    "    idx_sim = dict(zip(idx, sim))\n",
    "    idx_sim = sorted(idx_sim.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    similar_articles = idx_sim[:N]\n",
    "    articles = [art[0] for art in similar_articles]\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d620ab80-41f1-438c-9f10-f9a5bdcaf590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[369, 542, 194, 164, 159, 284, 153, 360, 382, 265]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_links(G = tp_nx, df = emb_df, article_id = 1, N = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab008511-b524-4207-9177-f4cca968fdc6",
   "metadata": {},
   "source": [
    "## Modelling Based Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61708a7e-18c4-4cb3-b6e8-74bc2446b13d",
   "metadata": {},
   "source": [
    "In this section we're going to build a binary classifier to predict the probability of a pair of edges to be connected or not. To do this we first need to identify all pairs of nodes which can form an edge, and identify the subset of those pairs which have already have an edge between them in the original network. We can then combine the embeddings (through vector addition) associated to all possible edges and pass that into a train test split function to generate training and testing partitions to pass into a classification model. \n",
    "\n",
    "The model I'll be using for this tutorial will be a gradient boosting classifier, for your own problems & experiments I advise you to try out a variety of different classifiers and selected the overall best performing one. We are building a binary classifier, however in almost all situations regarldess of the input data / network there will be a class imbalance on this classifier. Since we're taking all possible cominbations of edges which can be formed in the network, that result is N^N (where N is the number of nodes in the network), this is exponentially large. The only way that this wouldn't be a binary classifier is if your graph is already almost fully connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8eb0724-c0fa-4124-966e-587ef2a0e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_nodes = list(tp_nx.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9994925c-c38d-4681-af7d-880398bdb0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 ms, sys: 4.97 ms, total: 28 ms\n",
      "Wall time: 27.9 ms\n"
     ]
    }
   ],
   "source": [
    "%time all_possible_edges = [(x,y) for (x,y) in product(unique_nodes, unique_nodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e275d29-759e-44d6-bdd2-7c18435f893e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324900"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_possible_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24f60661-7621-45b4-972c-8f571744a9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 707 ms, sys: 18.4 ms, total: 725 ms\n",
      "Wall time: 725 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "edge_features = [\n",
    "    (mdl.wv.get_vector(str(i)) + mdl.wv.get_vector(str(j))) for i,j in all_possible_edges\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "323cde89-2c53-4ccb-8365-064624819fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = list(tp_nx.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bdf3789-8348-4c8b-86e9-5e8fa8f09797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 33s, sys: 674 ms, total: 2min 33s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%time is_con = [1 if e in edges else 0 for e in all_possible_edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fa3a543-06db-4a37-886e-f1e3cb6faf64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26178"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(is_con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fe205e-cc61-4f91-acce-449792808bb0",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d16a8551-a343-4807-bab1-393e1293fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training and target data\n",
    "X = np.array(edge_features)\n",
    "y = is_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "874593a1-0f94-44b4-b758-485171ed0af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2309d219-ecc9-46b0-9fcb-03d9a7afb64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 45s, sys: 325 ms, total: 1min 45s\n",
      "Wall time: 1min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# GBC classifier\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "# train the model\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d1153-1e48-499e-9dc0-122c3425a705",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ea4a6-2700-492b-9916-2afd5161b4b8",
   "metadata": {},
   "source": [
    "When dealing with class imbalances we can't look at traditional accuracy measures to evaluate the performance of the model. This is because if the model continously predicts the class which has the greater amount of data, then regardless of the model never predicting the other class, it will still achieve a high level of accuracy. The way to combat this is to use measures like Matthews Correlation Coefficient. \n",
    "\n",
    "```quote\n",
    "The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient.\n",
    "```\n",
    "- https://en.wikipedia.org/wiki/Phi_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0022514-47e6-4f97-919d-c951fde21abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test)\n",
    "y_true = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70f235e0-9c64-4474-90c6-ccd11d781363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy :  0.9165076433774495\n",
      "Training Accuracy :  0.9200281405267555\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(x_test)\n",
    "x_pred = clf.predict(x_train)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "train_acc = accuracy_score(y_train, x_pred)\n",
    "print(\"Testing Accuracy : \", test_acc)\n",
    "print(\"Training Accuracy : \", train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af2e8de6-4636-4443-95e8-d7e4c0c1691c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC Score :  0.29029414726539865\n"
     ]
    }
   ],
   "source": [
    "print(\"MCC Score : \", matthews_corrcoef(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67a09428-6855-4737-8041-4caead755055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[87518  6131]\n",
      " [ 2007  1814]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d2a460b-e039-45a9-ac25-411dae6181ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96     89525\n",
      "           1       0.47      0.23      0.31      7945\n",
      "\n",
      "    accuracy                           0.92     97470\n",
      "   macro avg       0.70      0.60      0.63     97470\n",
      "weighted avg       0.90      0.92      0.90     97470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Classification Report : \")\n",
    "print(classification_report(y_test, clf.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463d29d4-49ad-43c9-b394-cc3088009dbe",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3d929e1-d590-4ed7-b835-b21e60d9008c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ft = [(mdl.wv.get_vector(str('42'))+mdl.wv.get_vector(str('210')))]\n",
    "print(clf.predict(pred_ft)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a4d0caa-ea03-4994-b670-73d774527d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98561829, 0.01438171]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clf.predict_proba(pred_ft))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5146010-f35c-4f23-a8e4-5882cb0cbce3",
   "metadata": {},
   "source": [
    "Clearly as you can see that this is a poor performing model for it's given task (reflected by the MCC, ft-score, recall and percision). But that's alright since this article was simply for an educational purpose. Not every approach you use to solve a given problem will pan out or work, often times its not at fault of the approach but rather the data. In the case of this tutorial, the data was definetly at fault since I was only using a small sample of a actual research network which would be available if I scraped Arxiv for more data. But doing that would also increase the computational complexity to a lot of different components of this article (like running node2vec, generating all possible pairs of edges, training the model, etc.). \n",
    "\n",
    "The rule of thumb most data scientists follow when solving problems is that if the simple solutions work, then the more complex solutions should also work. Simpler solutions in recommendation systems involving collaborative filtering / content based are often quite easy to implement and yield relatively informative results and indications whether or not a more complex solution would work for solving this problem as well. Its often not the best to throw a neural network at the problem (like node2vec) for a variety of reasons (like training time, interpretability, model inference time, etc.). \n",
    "\n",
    "For a guideline on how to implement the simpler solutions in recommendation systems, you can refer to another article I've written [here](). Furthermore, if you want to follow through in the jupyter notebook associated to this project, you can reference my [GitHub]()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b67e10-927d-4fc7-bc3b-0ef3115f7c19",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
