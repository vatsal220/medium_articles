{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f17e3f-fcce-4aec-93f9-ad76897323cf",
   "metadata": {},
   "source": [
    "# Link Prediction w/ n2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c26a7c0-135c-40c7-84af-16260c96793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77beb5bb-e6db-4d92-88fb-2ee138c24836",
   "metadata": {},
   "source": [
    "You can install the arxiv package in Python with the following command:  \n",
    "`pip install arxiv`  \n",
    "or follow the instructions here : https://pypi.org/project/arxiv/  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53441e5-efb1-4add-9c77-97f08a30f6de",
   "metadata": {},
   "source": [
    "## What is Link Prediction?\n",
    "\n",
    "\n",
    "\n",
    "## Cold Start Problem in Recommendation Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe6b94e0-f903-422f-baab-76010e772026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import arxiv\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from node2vec import Node2Vec as n2v\n",
    "from node2vec.edges import HadamardEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5116e841-87c8-4e79-9c1f-6b740c7a9ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "queries = [\n",
    "    'automl', 'machinelearning', 'data', 'phyiscs','mathematics', 'recommendation system', 'nlp', 'neural networks'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b445bcd4-ecdd-4b72-84ff-e36afc5e1a2c",
   "metadata": {},
   "source": [
    "# Fetch Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4e4258-9565-41b0-a01a-4ef1cf783c01",
   "metadata": {},
   "source": [
    "We want to hit th Arxiv API to gather some information about the latest research papers based on the queries we've identified above. This will allow us to then create a network from this research paper data and then we can try to predict links on that network. For the purposes of this article, I will search for a maximum of 1000 results per query, but you don't have to set yourself to the same constraints. The Arxiv API allows users to hit up to 300,000 results per query. The function outlined below will generate a CSV fetching the following information :   \n",
    "```'title', 'date', 'article_id', 'url', 'main_topic', 'all_topics', 'authors', 'year'```   \n",
    "You are able to fetch more information like the `links, summary, article` but I decided not to since those features won't really be used for the purposes of this analysis and tutorial.\n",
    "\n",
    "For reference to the Arxiv API, you can find their detailed documentation here : https://arxiv.org/help/api/user-manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14298db4-2590-4992-a49c-d6c93d962b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv(queries, max_results = 1000):\n",
    "    '''\n",
    "    This function will search arxiv associated to a set of queries and store\n",
    "    the latest 10000 (max_results) associated to that search.\n",
    "    \n",
    "    params:\n",
    "        queries (List -> Str) : A list of strings containing keywords you want\n",
    "                                to search on Arxiv\n",
    "        max_results (Int) : The maximum number of results you want to see associated\n",
    "                            to your search. Default value is 1000, capped at 300000\n",
    "                            \n",
    "    returns:\n",
    "        This function will return a DataFrame holding the following columns associated\n",
    "        to the queries the user has passed. \n",
    "            `title`, `date`, `article_id`, `url`, `main_topic`, `all_topics`\n",
    "    \n",
    "    example:\n",
    "        research_df = search_arxiv(\n",
    "            queries = ['automl', 'recommender system', 'nlp', 'data science'],\n",
    "            max_results = 10000\n",
    "        )\n",
    "    '''\n",
    "    d = []\n",
    "    searches = []\n",
    "    # hitting the API\n",
    "    for query in queries:\n",
    "        search = arxiv.Search(\n",
    "          query = query,\n",
    "          max_results = max_results,\n",
    "          sort_by = arxiv.SortCriterion.SubmittedDate,\n",
    "          sort_order = arxiv.SortOrder.Descending\n",
    "        )\n",
    "        searches.append(search)\n",
    "    \n",
    "    # Converting search result into df\n",
    "    for search in searches:\n",
    "        for res in search.results():\n",
    "            data = {\n",
    "                'title' : res.title,\n",
    "                'date' : res.published,\n",
    "                'article_id' : res.entry_id,\n",
    "                'url' : res.pdf_url,\n",
    "                'main_topic' : res.primary_category,\n",
    "                'all_topics' : res.categories,\n",
    "                'authors' : res.authors\n",
    "            }\n",
    "            d.append(data)\n",
    "        \n",
    "    d = pd.DataFrame(d)\n",
    "    d['year'] = pd.DatetimeIndex(d['date']).year\n",
    "    \n",
    "    # change article id from url to integer\n",
    "    unique_article_ids = d.article_id.unique()\n",
    "    article_mapping = {art:idx for idx,art in enumerate(unique_article_ids)}\n",
    "    d['article_id'] = d['article_id'].map(article_mapping)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f77985-1287-4641-bd45-9dbd873c0da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.24 s, sys: 472 ms, total: 8.71 s\n",
      "Wall time: 3min 48s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5332, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "research_df = search_arxiv(\n",
    "    queries = queries,\n",
    "    max_results = 1000\n",
    ")\n",
    "research_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e278a-23de-4671-8bd4-853636941495",
   "metadata": {},
   "source": [
    "If you're having trouble querying the data, for reproducibility purposes, the CSV I used for the analysis conducted in this article was uploaded to my GitHub which you can find here. https://github.com/vatsal220/medium_articles/blob/main/link_prediction/data/arxiv_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b32ed-3c68-412a-bf34-798292d47684",
   "metadata": {},
   "source": [
    "## Generate Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b2d6f-60f9-4568-afc2-50cb32154cff",
   "metadata": {},
   "source": [
    "Now that we've fetched the data using the Arxiv API, we can generate a network. The network will have the following structure, nodes will be the article_ids and the edges will be all topics connecting a pair of articles. For example, article_id 1 with the following topics `astro-physics, and stats` can be connected to article_id 10 with the topic `stats` and article_id 7 with the topics `astro-physics, math`. This will be a multi-edge network where each edge will hold a weight of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ef8e9a-0c80-4d4b-a56b-7e50263abacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network(df, node_col = 'article_id', edge_col = 'main_topic'):\n",
    "    '''\n",
    "    -----------------------------------DEPRICATED-----------------------------------\n",
    "    This function will generate a article to article network given an input DataFrame.\n",
    "    It will do so by creating an edge_dictionary where each key is going to be a node\n",
    "    referenced by unique values in node_col and the values will be a list of other nodes\n",
    "    connected to the key through the edge_col.\n",
    "    \n",
    "    params:\n",
    "        df (DataFrame) : The dataset which holds the node and edge columns\n",
    "        node_col (String) : The column name associated to the nodes of the network\n",
    "        edge_col (String) : The column name associated to the edges of the network\n",
    "        \n",
    "    returns:\n",
    "        A networkx graph corresponding to the input dataset\n",
    "        \n",
    "    example:\n",
    "        generate_network(\n",
    "            research_df,\n",
    "            node_col = 'article_id',\n",
    "            edge_col = 'main_topic'\n",
    "        )\n",
    "    '''\n",
    "    edge_dct = {}\n",
    "    for i,g in df.groupby(node_col):\n",
    "        topics = df[edge_col].unique()\n",
    "        edge_df = df[(df[node_col] != i) & (df[edge_col].isin(topics))]\n",
    "        edges = list(edge_df[node_col].unique())\n",
    "        edge_dct[i] = edges\n",
    "    \n",
    "    # create nx network\n",
    "    g = nx.Graph(edge_dct)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f78df9a9-ff85-41b4-a81e-fd633f5f241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network(df, node_col = 'article_id', edge_col = 'all_topics'):\n",
    "    '''\n",
    "    This function will generate a article to article network given an input DataFrame.\n",
    "    It will do so by creating the adjacency matrix associated to the network where the\n",
    "    columns and rows represent the node_col. A pair of nodes will be connected by an edge\n",
    "    based on the count of the edge_col connecting the two nodes. \n",
    "    \n",
    "    params:\n",
    "        df (DataFrame) : The dataset which holds the node and edge columns\n",
    "        node_col (String) : The column name associated to the nodes of the network\n",
    "        edge_col (String) : The column name associated to the edges of the network\n",
    "        nx_type (Function): The type of network you want to create, MultiGraph is \n",
    "                            default\n",
    "        \n",
    "    returns:\n",
    "        A networkx graph corresponding to the input dataset\n",
    "        \n",
    "    example:\n",
    "        generate_network(\n",
    "            research_df,\n",
    "            node_col = 'article_id',\n",
    "            edge_col = 'all_topics'\n",
    "        )\n",
    "    '''\n",
    "    df = df.explode(edge_col)\n",
    "    df[node_col + '_copied'] = df[node_col]\n",
    "    \n",
    "    # generate adjacency matrix\n",
    "    adj_m = df.pivot_table(\n",
    "        index = node_col,\n",
    "        columns = node_col + '_copied',\n",
    "        values = edge_col,\n",
    "        aggfunc = 'count'\n",
    "    ).fillna(0).values\n",
    "    \n",
    "    # convert matrix into a multi\n",
    "    G = nx.convert_matrix.from_numpy_array(adj_m)\n",
    "    return adj_m, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6878f12-a0bc-4070-8fca-689b01154970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 541 ms, sys: 222 ms, total: 763 ms\n",
      "Wall time: 817 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "adj, research_network = generate_network(\n",
    "    research_df, \n",
    "    node_col = 'article_id', \n",
    "    edge_col = 'main_topic'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f8ca352-20f3-4358-95c7-2bcf444d09eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 4513\n",
      "Number of edges: 4513\n",
      "Average degree:   2.0000\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(research_network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f74b0208-338a-4983-8342-b2f12ddb216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame.from_dict({\n",
    "    'a1' : [0,0,0,1,2,3,4,5],\n",
    "    'a2' : [0,0,01,2,3,4,5],\n",
    "    'tp' : ['cs', 'cs', 'cl', 'mth', 'eng', 'phy']\n",
    "}, orient = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a65e5050-40d5-4439-862c-f2a601b1a086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>a2</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "a2    0    1    2    3    4    5\n",
       "a1                              \n",
       "0   1.0  NaN  NaN  NaN  NaN  NaN\n",
       "1   NaN  1.0  NaN  NaN  NaN  NaN\n",
       "2   NaN  NaN  1.0  NaN  NaN  NaN\n",
       "3   NaN  NaN  NaN  1.0  NaN  NaN\n",
       "4   NaN  NaN  NaN  NaN  1.0  NaN\n",
       "5   NaN  NaN  NaN  NaN  NaN  1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.pivot_table(\n",
    "    index = 'a1',\n",
    "    columns = 'a2',\n",
    "    values = 'tp',\n",
    "    aggfunc = 'count'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6d5d7cd-2442-4fcc-83e0-467fdbb9e274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>article_id</th>\n",
       "      <th>url</th>\n",
       "      <th>main_topic</th>\n",
       "      <th>all_topics</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Review of automated time series forecasting pi...</td>\n",
       "      <td>2022-02-03 17:26:27+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>http://arxiv.org/pdf/2202.01712v1</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>[cs.LG]</td>\n",
       "      <td>[Stefan Meisenbacher, Marian Turowski, Kaleb P...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hubble Asteroid Hunter: I. Identifying asteroi...</td>\n",
       "      <td>2022-02-01 06:56:20+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>http://arxiv.org/pdf/2202.00246v1</td>\n",
       "      <td>astro-ph.EP</td>\n",
       "      <td>[astro-ph.EP, astro-ph.IM]</td>\n",
       "      <td>[Sandor Kruk, Pablo García Martín, Marcel Pope...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NAS-Bench-Suite: NAS Evaluation is (Now) Surpr...</td>\n",
       "      <td>2022-01-31 18:02:09+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>http://arxiv.org/pdf/2201.13396v2</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>[cs.LG, cs.AI, stat.ML]</td>\n",
       "      <td>[Yash Mehta, Colin White, Arber Zela, Arjun Kr...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Online AutoML: An adaptive AutoML framework fo...</td>\n",
       "      <td>2022-01-24 15:37:20+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>http://arxiv.org/pdf/2201.09750v1</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>[cs.LG, cs.AI]</td>\n",
       "      <td>[Bilge Celik, Prabhant Singh, Joaquin Vanschoren]</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Automated Reinforcement Learning (AutoRL): A S...</td>\n",
       "      <td>2022-01-11 12:41:43+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>http://arxiv.org/pdf/2201.03916v1</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>[cs.LG]</td>\n",
       "      <td>[Jack Parker-Holder, Raghu Rajan, Xingyou Song...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5327</th>\n",
       "      <td>Deep Impulse Responses: Estimating and Paramet...</td>\n",
       "      <td>2022-02-07 18:57:23+00:00</td>\n",
       "      <td>4508</td>\n",
       "      <td>http://arxiv.org/pdf/2202.03416v1</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>[cs.SD, cs.LG, eess.AS]</td>\n",
       "      <td>[Alexander Richard, Peter Dodds, Vamsi Krishna...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5328</th>\n",
       "      <td>PopNet: Real-Time Population-Level Disease Pre...</td>\n",
       "      <td>2022-02-07 18:55:46+00:00</td>\n",
       "      <td>4509</td>\n",
       "      <td>http://arxiv.org/pdf/2202.03415v1</td>\n",
       "      <td>cs.SI</td>\n",
       "      <td>[cs.SI]</td>\n",
       "      <td>[Junyi Gao, Cao Xiao, Lucas M. Glass, Jimeng Sun]</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5329</th>\n",
       "      <td>Investigating the fidelity of explainable arti...</td>\n",
       "      <td>2022-02-07 18:47:15+00:00</td>\n",
       "      <td>4510</td>\n",
       "      <td>http://arxiv.org/pdf/2202.03407v1</td>\n",
       "      <td>physics.geo-ph</td>\n",
       "      <td>[physics.geo-ph, cs.AI, cs.LG]</td>\n",
       "      <td>[Antonios Mamalakis, Elizabeth A. Barnes, Imme...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5330</th>\n",
       "      <td>Dependence model assessment and selection with...</td>\n",
       "      <td>2022-02-07 18:44:39+00:00</td>\n",
       "      <td>4511</td>\n",
       "      <td>http://arxiv.org/pdf/2202.03406v1</td>\n",
       "      <td>stat.ML</td>\n",
       "      <td>[stat.ML, cs.LG, q-fin.CP, q-fin.RM, stat.AP, ...</td>\n",
       "      <td>[Marius Hofert, Avinash Prasad, Mu Zhu]</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331</th>\n",
       "      <td>Private Read Update Write (PRUW) with Storage ...</td>\n",
       "      <td>2022-02-07 18:38:46+00:00</td>\n",
       "      <td>4512</td>\n",
       "      <td>http://arxiv.org/pdf/2202.03400v1</td>\n",
       "      <td>cs.IT</td>\n",
       "      <td>[cs.IT, cs.CR, cs.NI, eess.SP, math.IT]</td>\n",
       "      <td>[Sajani Vithana, Sennur Ulukus]</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5332 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     Review of automated time series forecasting pi...   \n",
       "1     Hubble Asteroid Hunter: I. Identifying asteroi...   \n",
       "2     NAS-Bench-Suite: NAS Evaluation is (Now) Surpr...   \n",
       "3     Online AutoML: An adaptive AutoML framework fo...   \n",
       "4     Automated Reinforcement Learning (AutoRL): A S...   \n",
       "...                                                 ...   \n",
       "5327  Deep Impulse Responses: Estimating and Paramet...   \n",
       "5328  PopNet: Real-Time Population-Level Disease Pre...   \n",
       "5329  Investigating the fidelity of explainable arti...   \n",
       "5330  Dependence model assessment and selection with...   \n",
       "5331  Private Read Update Write (PRUW) with Storage ...   \n",
       "\n",
       "                          date  article_id                                url  \\\n",
       "0    2022-02-03 17:26:27+00:00           0  http://arxiv.org/pdf/2202.01712v1   \n",
       "1    2022-02-01 06:56:20+00:00           1  http://arxiv.org/pdf/2202.00246v1   \n",
       "2    2022-01-31 18:02:09+00:00           2  http://arxiv.org/pdf/2201.13396v2   \n",
       "3    2022-01-24 15:37:20+00:00           3  http://arxiv.org/pdf/2201.09750v1   \n",
       "4    2022-01-11 12:41:43+00:00           4  http://arxiv.org/pdf/2201.03916v1   \n",
       "...                        ...         ...                                ...   \n",
       "5327 2022-02-07 18:57:23+00:00        4508  http://arxiv.org/pdf/2202.03416v1   \n",
       "5328 2022-02-07 18:55:46+00:00        4509  http://arxiv.org/pdf/2202.03415v1   \n",
       "5329 2022-02-07 18:47:15+00:00        4510  http://arxiv.org/pdf/2202.03407v1   \n",
       "5330 2022-02-07 18:44:39+00:00        4511  http://arxiv.org/pdf/2202.03406v1   \n",
       "5331 2022-02-07 18:38:46+00:00        4512  http://arxiv.org/pdf/2202.03400v1   \n",
       "\n",
       "          main_topic                                         all_topics  \\\n",
       "0              cs.LG                                            [cs.LG]   \n",
       "1        astro-ph.EP                         [astro-ph.EP, astro-ph.IM]   \n",
       "2              cs.LG                            [cs.LG, cs.AI, stat.ML]   \n",
       "3              cs.LG                                     [cs.LG, cs.AI]   \n",
       "4              cs.LG                                            [cs.LG]   \n",
       "...              ...                                                ...   \n",
       "5327           cs.SD                            [cs.SD, cs.LG, eess.AS]   \n",
       "5328           cs.SI                                            [cs.SI]   \n",
       "5329  physics.geo-ph                     [physics.geo-ph, cs.AI, cs.LG]   \n",
       "5330         stat.ML  [stat.ML, cs.LG, q-fin.CP, q-fin.RM, stat.AP, ...   \n",
       "5331           cs.IT            [cs.IT, cs.CR, cs.NI, eess.SP, math.IT]   \n",
       "\n",
       "                                                authors  year  \n",
       "0     [Stefan Meisenbacher, Marian Turowski, Kaleb P...  2022  \n",
       "1     [Sandor Kruk, Pablo García Martín, Marcel Pope...  2022  \n",
       "2     [Yash Mehta, Colin White, Arber Zela, Arjun Kr...  2022  \n",
       "3     [Bilge Celik, Prabhant Singh, Joaquin Vanschoren]  2022  \n",
       "4     [Jack Parker-Holder, Raghu Rajan, Xingyou Song...  2022  \n",
       "...                                                 ...   ...  \n",
       "5327  [Alexander Richard, Peter Dodds, Vamsi Krishna...  2022  \n",
       "5328  [Junyi Gao, Cao Xiao, Lucas M. Glass, Jimeng Sun]  2022  \n",
       "5329  [Antonios Mamalakis, Elizabeth A. Barnes, Imme...  2022  \n",
       "5330            [Marius Hofert, Avinash Prasad, Mu Zhu]  2022  \n",
       "5331                    [Sajani Vithana, Sennur Ulukus]  2022  \n",
       "\n",
       "[5332 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dda3a2d-de61-4033-9d08-fd376b5ae618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a15484-08b0-4a80-9c25-6f156a7ef996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e19cf4-f3aa-4912-a6bd-14800bcac5c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66415397-621f-4ae9-9e2c-388359e33478",
   "metadata": {},
   "source": [
    "## Node2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97bc9ab-f39b-4ef8-833c-6cbe7e1e65ef",
   "metadata": {},
   "source": [
    "This component will cover running node2vec on the graph generated above and creating the associated node embeddings for that network. These embeddings will play a crucial role coming up as they're the main features necessary for building a link prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066bf93a-ad52-491b-adb1-79fe6481ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time g_emb = n2v(research_network, dimensions=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca8a21-5aa8-493a-ba57-e83695637ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 1 # Node2Vec fit window\n",
    "MIN_COUNT = 1 # Node2Vec min. count\n",
    "BATCH_WORDS = 4 # Node2Vec batch words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce72cd-db36-430f-b65e-f7b9b9604120",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = g_emb.fit(\n",
    "    window=WINDOW,\n",
    "    min_count=MIN_COUNT,\n",
    "    batch_words=BATCH_WORDS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda734d7-6b08-4a76-aca9-6a54948c1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_node = '1'\n",
    "for s in mdl.wv.most_similar(input_node, topn = 10):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a904f-fc1c-4a60-972c-94f39143f64d",
   "metadata": {},
   "source": [
    "## Generate Embeddings DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27f5562-4eb0-48f5-91a0-877edbe2c8ad",
   "metadata": {},
   "source": [
    "HammardEmbedder is a built in function in the Node2Vec module which will map each edge in the graph to a 2 dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a2653-a814-4915-b217-904e31cf024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_embs = HadamardEmbedder(keyed_vectors=mdl.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a284bc-6b3d-45cf-a6d7-e5a638706acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[1,2]\n",
    "edges_embs[str(x[0]),str(x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f66aff-fe52-4ba1-85e4-32d9ec1cb5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = (\n",
    "    pd.DataFrame(\n",
    "        [mdl.wv.get_vector(str(n)) for n in research_network.nodes()],\n",
    "        index = research_network.nodes\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca946a47-125a-41a5-9a36-bfd1543d6ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd6bb7-c281-47a8-a62c-3970845ef989",
   "metadata": {},
   "source": [
    "## Recommendations w/ Distance Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0017cc-caf6-40b2-ad3c-3aaca18d4994",
   "metadata": {},
   "source": [
    "Now that we have a embedding vector representing each node in the network, we can then use distance measures like cosine similarity, euclidean distance, manhattan distance, etc. to measure the amount of distance between nodes. The assumption that we're making by using these distance measures is that nodes in close proximity with each other should also have an edge connecting each other. This is a good assumption to make as node2vec tries to preserve the initial structure of the original input graph. Now we can essentially write out code to measure similarity levels between two vectors using cosine similarity (or a different distance measure) and identify pairs of nodes which don't currently have an edge between them but do have a large similarity should create an edge between them. This interpretation can be different for multi / weighted / directed graphs. Pick and use a similarity measure appropriate to the network and problem you're trying to solve. Also be aware that different measures have different interperations, for this problem you want to pick maximal cosine similarity scores whereas if you were to use something like euclidean distance, you would want to pick the minimal distance between two vectors.\n",
    "\n",
    "On a side note, I do want to mention that the curse of dimensionality is rampant when solving these types of problems. It's especially problematic when using euclidean distance in particular to measure the distance between vectors in higher dimensions. The term higher dimensions is broad and open to interpretation, the threshold for a dimension to be \"high\" is not strictly defined and varies from problem to problem. Without going to deep into the mathematics behind things, euclidean distance is not a good measure to use for sparse or high dimensional vectors. You can reference this post on stack exchange which outlines the mathematical reasoning as to why this is the case. \n",
    "\n",
    "- https://stats.stackexchange.com/questions/29627/euclidean-distance-is-usually-not-good-for-sparse-data-and-more-general-case\n",
    "- https://stats.stackexchange.com/questions/99171/why-is-euclidean-distance-not-a-good-metric-in-high-dimensions\n",
    "\n",
    "For more on the curse of dimensionality, refer to this [paper](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) by the Computer Science department from the University of Washington."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf712d2-f670-4d47-b60a-3854fe138bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_links(G, df, article_id, N):\n",
    "    '''\n",
    "    This function will predict the top N links a node (article_id) should be connected with\n",
    "    which it is not already connected with in G.\n",
    "    \n",
    "    params:\n",
    "        G (Netowrkx Graph) : The network used to create the embeddings\n",
    "        df (DataFrame) : The dataframe which has embeddings associated to each node\n",
    "        article_id (Integer) : The article you're interested \n",
    "        N (Integer) : The number of recommended links you want to return\n",
    "        \n",
    "    returns:\n",
    "        This function will return a list of nodes the input node should be connected with.\n",
    "    '''\n",
    "    \n",
    "    # separate target article with all others\n",
    "    all_nodes = G.nodes()\n",
    "    other_nodes = [n for n in G.adj[article_id] if n not in all_nodes]\n",
    "    \n",
    "    article = df[df.index == article_id]\n",
    "    other_articles = df[df.index.isin(other_nodes)]\n",
    "    \n",
    "    # get similarity of current reader and all other readers\n",
    "    sim = cosine_similarity(article, other_articles)[0].tolist()\n",
    "    idx = other_articles.index.tolist()\n",
    "    \n",
    "    # create a similarity dictionary for this user w.r.t all other users\n",
    "    idx_sim = dict(zip(idx, sim))\n",
    "    idx_sim = sorted(idx_sim.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    similar_articles = idx_sim[:N]\n",
    "    articles = [art[0] for art in similar_articles]\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620ab80-41f1-438c-9f10-f9a5bdcaf590",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_links(G = research_network, df = emb_df, article_id = 1, N = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a9cb85-4ff0-444e-a2c3-28a66fe535e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(research_network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b00f6aa-9f07-4b42-ac45-325329d2c6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_network.has_edge(,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f137e636-321f-4ac3-818b-ef980adb3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = research_network.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ac61f186-f37e-4818-8b52-ad2e47b3cc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n in research_network.adj[569] if n not in all_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1515c980-0f87-4943-8f46-71eacdb4cdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(research_network.adj[a]) for a in range(0, 570) if len(research_network.adj[a]) != 569]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b9e2a-5539-44ad-8a41-15553a1e3722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab008511-b524-4207-9177-f4cca968fdc6",
   "metadata": {},
   "source": [
    "## Modelling Based Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722fda8-c32a-4a0c-997a-4d3dd359f108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45fe205e-cc61-4f91-acce-449792808bb0",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a8551-a343-4807-bab1-393e1293fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2309d219-ecc9-46b0-9fcb-03d9a7afb64d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "880d1153-1e48-499e-9dc0-122c3425a705",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a09428-6855-4737-8041-4caead755055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae8e831-6341-49d6-883a-7b4761f2a789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "463d29d4-49ad-43c9-b394-cc3088009dbe",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d929e1-d590-4ed7-b835-b21e60d9008c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73e4de5-9e5e-4ef8-8cb8-0adb6fd95cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67b67e10-927d-4fc7-bc3b-0ef3115f7c19",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b89e85f-bee0-41fc-97f4-5aa565d84479",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.convert_matrix.from_numpy_array(arr, create_using = nx.DiGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "856c484a-2cd4-4628-a724-511b2b197771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 2\n",
      "Number of edges: 4\n",
      "Average in degree:   2.0000\n",
      "Average out degree:   2.0000\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5c51e77-9461-4810-959c-70601813d687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nx.draw(g, with_labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78010569-6bb7-4122-8d19-0a6764513bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = np.array([[0,2,3,0], [2,0,1,1], [3,1,0,0], [0,1,0,0]])\n",
    "mat2 = np.array([[0,4,5,3], [4,0,1,2], [5,1,0,0], [3,2,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "22e22ca1-cf2b-4e2c-969f-e41b3af14c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23,  3,  2,  4],\n",
       "       [ 8, 11, 10,  6],\n",
       "       [ 4, 12, 16, 11],\n",
       "       [ 4,  0,  1,  2]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1@mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f5caedb-ee93-4ed3-a2ec-62238443cc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23,  3,  2,  4],\n",
       "       [ 8, 11, 10,  6],\n",
       "       [ 4, 12, 16, 11],\n",
       "       [ 4,  0,  1,  2]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1.dot(mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af53ccb0-9ab3-4e72-9faf-0654e324a227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  8, 15,  0],\n",
       "       [ 8,  0,  1,  2],\n",
       "       [15,  1,  0,  0],\n",
       "       [ 0,  2,  0,  0]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1*mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f7cabe8-1e62-40df-b705-9fe404ca397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = np.array([[0,2], [2,0]])\n",
    "mat2 = np.array([[0,4], [4,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ecaf7a5-62b3-4fa4-811f-2fa154a5af02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 0],\n",
       "       [0, 8]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1@mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b943190d-b470-46bd-b248-50addf4d0033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 8],\n",
       "       [8, 0]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1*mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417dd5a3-c63f-47c8-865e-7de2264c0082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d250cc32-6b61-442b-9b7d-bf24c5c2b9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 2, 4],\n",
       "       [1, 3, 6],\n",
       "       [4, 8, 6]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_org = np.array(\n",
    "    [\n",
    "        [1,4,5,6,8,7,6,5,4,2,3,5],\n",
    "        [2,3,4,6,2,1,4,4,6,3,2,1],\n",
    "        [0,1,2,5,3,5,6,4,3,2,6,2],\n",
    "        [0,7,5,7,2,3,4,5,6,4,2,1],\n",
    "        [0,1,2,5,3,5,6,4,3,2,6,2],\n",
    "        [0,1,2,5,3,5,6,4,3,2,6,2],\n",
    "        [1,4,5,6,8,7,6,5,4,2,3,5],\n",
    "        [0,1,2,5,3,5,6,4,3,2,6,2]\n",
    "    ]\n",
    ")\n",
    "\n",
    "def create_subarray(arr, r_idx, c_idx):\n",
    "    try:\n",
    "        return arr[np.ix_(r_idx, c_idx)]\n",
    "    except:\n",
    "        raise IndexError(\"Index is out of bounds\")\n",
    "\n",
    "s1 = [1,4,6]\n",
    "create_subarray(m_org, s1, s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8d423304-a69b-4e24-8a5e-dac95e5d7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = [1,4,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fbab4afd-9da6-4bd7-bb1c-c3ad580ef5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 2, 4],\n",
       "       [1, 3, 6],\n",
       "       [4, 8, 6]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_org[np.asarray(s1)[:, None], s1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc64011-848e-4263-b524-6775477d46f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f6d79-92da-481a-a7e5-22e307f6fbca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
