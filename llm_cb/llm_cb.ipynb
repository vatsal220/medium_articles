{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efdb1e13-a7b7-47c9-a55a-8466bfabec9d",
   "metadata": {},
   "source": [
    "# LLama LLM\n",
    "\n",
    "The recent buzz around LLMs (large language models) is justified from the significant boost in noticable performance on common tasks in NLP. Recently, Meta research open sourced Llama 2.0, a large language model which outperformed GPT on several benchmark datasets. They released Llama 2.0 \n",
    "\n",
    "\n",
    "This release includes model weights and starting code for pretrained and fine-tuned Llama language models — ranging from 7B to 70B parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91841c29-797b-48de-b027-289e00c09c37",
   "metadata": {},
   "source": [
    "## Download Model\n",
    "[LLama2 - HuggingFace](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/blob/main/llama-2-7b-chat.ggmlv3.q4_0.bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f82e15a8-b74e-4316-ae74-c251e5a1cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48dbbe64-4291-43db-afc8-138388970d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9466fa87-846b-4589-84aa-1c54b4a72ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b3dff6-eaeb-4913-b9eb-3685ed0e35e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: Find if the number is a prime number: {number}. \n",
    "\n",
    "Output : Give step by step explanation for the answer\n",
    "\n",
    "Answer: Inform if the number is prime or not prime\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"number\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a94c1df-d830-4590-ba1d-e0604e7de0bb",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "653bdfc0-fc45-4b42-984f-8f020dabce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# Verbose is required to pass to the callback manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f333cf7f-2316-409d-9704-36321cf20bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/vatsalpatel/Downloads/llama-2-7b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 3917.73 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/vatsalpatel/Downloads/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "    metadata={\"temperature\": 0, \"max_length\": 200000, \"top_p\": 1},\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdf25f7-2834-49a3-9955-98eb2b1c35ca",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11dc82ae-ac95-460f-9bd3-d8f20c58b52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". If it's a prime number, explain why it's prime; otherwise, explain why it's not prime.\n",
      "\n",
      "Input : The number to be checked for primality is 3.\n",
      "\n",
      "Explanation : To determine whether a given number is prime, we can use the following steps:\n",
      "Step 1: Check if the number is equal to 1 or 0. If it is, then it's not a prime number because 1 and 0 are not prime numbers.\n",
      "Step 2: Check if the number is divisible by 2. If it is, then it's not a prime number because 2 is the smallest number that can divide it exactly, and therefore, it cannot be divided by any other number except for 1 and itself.\n",
      "Step 3: Check if the number is divisible by any other numbers besides 1 and itself. If it is, then it's not a prime number because it can be divided by at least one other number besides 1 and itself.\n",
      "In the case of the number 3, we can see that it passes step 1 and step 2, but fails step 3. Therefore, we conclude that 3 is a prime number"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  6578.50 ms\n",
      "llama_print_timings:      sample time =   183.19 ms /   256 runs   (    0.72 ms per token,  1397.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8042.48 ms /    40 tokens (  201.06 ms per token,     4.97 tokens per second)\n",
      "llama_print_timings:        eval time = 12355.89 ms /   256 runs   (   48.27 ms per token,    20.72 tokens per second)\n",
      "llama_print_timings:       total time = 21049.26 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\". If it's a prime number, explain why it's prime; otherwise, explain why it's not prime.\\n\\nInput : The number to be checked for primality is 3.\\n\\nExplanation : To determine whether a given number is prime, we can use the following steps:\\nStep 1: Check if the number is equal to 1 or 0. If it is, then it's not a prime number because 1 and 0 are not prime numbers.\\nStep 2: Check if the number is divisible by 2. If it is, then it's not a prime number because 2 is the smallest number that can divide it exactly, and therefore, it cannot be divided by any other number except for 1 and itself.\\nStep 3: Check if the number is divisible by any other numbers besides 1 and itself. If it is, then it's not a prime number because it can be divided by at least one other number besides 1 and itself.\\nIn the case of the number 3, we can see that it passes step 1 and step 2, but fails step 3. Therefore, we conclude that 3 is a prime number\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm,prompt = prompt)\n",
    "chain.run(\"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74d01cfb-3a58-4943-8ceb-60cbef62c540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "его explanation\n",
      "\n",
      "BERT, or Bidirectional Encoder Representations from Transformers, is a type of machine learning model that can understand text. It's like a superhero for words!\n",
      "\n",
      "To make BERT work, we need to do some magic with computers. First, we take a big pile of words and feed them into a special kind of computer called a transformer. The transformer is like a big chewy machine that chews on the words and makes them into something special.\n",
      "Next, we give the transformed words to another computer called a decoder. The decoder is like a magic wand that can make new words out of the old ones. It's like turning a pile of building blocks into a really cool castle!\n",
      "But here's the coolest part: BERT can learn how to do all this on its own, without us having to tell it exactly what to do. It's like giving a robot a box of LEGOs and saying \"Make something awesome!\" And then, magic! The robot builds a whole city!\n",
      "So that's how BERT works! It's like a superhero for words, using magic computers to make text understandable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  6578.50 ms\n",
      "llama_print_timings:      sample time =   182.61 ms /   256 runs   (    0.71 ms per token,  1401.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =   977.12 ms /    23 tokens (   42.48 ms per token,    23.54 tokens per second)\n",
      "llama_print_timings:        eval time = 12185.41 ms /   255 runs   (   47.79 ms per token,    20.93 tokens per second)\n",
      "llama_print_timings:       total time = 13811.20 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'его explanation\\n\\nBERT, or Bidirectional Encoder Representations from Transformers, is a type of machine learning model that can understand text. It\\'s like a superhero for words!\\n\\nTo make BERT work, we need to do some magic with computers. First, we take a big pile of words and feed them into a special kind of computer called a transformer. The transformer is like a big chewy machine that chews on the words and makes them into something special.\\nNext, we give the transformed words to another computer called a decoder. The decoder is like a magic wand that can make new words out of the old ones. It\\'s like turning a pile of building blocks into a really cool castle!\\nBut here\\'s the coolest part: BERT can learn how to do all this on its own, without us having to tell it exactly what to do. It\\'s like giving a robot a box of LEGOs and saying \"Make something awesome!\" And then, magic! The robot builds a whole city!\\nSo that\\'s how BERT works! It\\'s like a superhero for words, using magic computers to make text understandable'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Question: Can you explain like I'm 5 how BERT works under the hood?\n",
    "\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e49c39c-84c9-40ce-abeb-eff0da1cc33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How can I do this?\n",
      "Here is an example of my dataframe:\n",
      "\n",
      "| org_id | account_id | account_name |\n",
      "| --- | --- | --- |\n",
      "| 1 | 101 | ABC Inc. |\n",
      "| 1 | 102 | XYZ Corporation |\n",
      "| 2 | 201 | DEF Company |\n",
      "| 2 | 202 | GHI Limited |\n",
      "\n",
      "I want to create a new column `workload` that will take the jaro similarity between account_name for each org and assign a workload of high, medium or low depending on the jaro score. I am not sure how to do this as I am not familiar with this type of analysis.\n",
      "Can anyone help me?\n",
      "Thanks in advance!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  6578.50 ms\n",
      "llama_print_timings:      sample time =   122.79 ms /   172 runs   (    0.71 ms per token,  1400.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3087.20 ms /    72 tokens (   42.88 ms per token,    23.32 tokens per second)\n",
      "llama_print_timings:        eval time =  8392.17 ms /   172 runs   (   48.79 ms per token,    20.50 tokens per second)\n",
      "llama_print_timings:       total time = 11915.83 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHow can I do this?\\nHere is an example of my dataframe:\\n\\n| org_id | account_id | account_name |\\n| --- | --- | --- |\\n| 1 | 101 | ABC Inc. |\\n| 1 | 102 | XYZ Corporation |\\n| 2 | 201 | DEF Company |\\n| 2 | 202 | GHI Limited |\\n\\nI want to create a new column `workload` that will take the jaro similarity between account_name for each org and assign a workload of high, medium or low depending on the jaro score. I am not sure how to do this as I am not familiar with this type of analysis.\\nCan anyone help me?\\nThanks in advance!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Question: Given a pandas dataframe with the following columns `org_id`, `account_id` and `account_name`, I want to create a new column `workload` which will take the jaro similarity associated with each account_names per org and assign a workload to accounts with a high jaro score greater than a threshold of 97%\n",
    "\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e4965bd-124a-456b-9777-24b7c3dc6df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "The issue is that the `model_path` parameter in the `LlamaCpp` constructor is not a valid file path. Check that the file path is correct and try again.\n",
      "I hope this helps! Let me know if you have any other questions."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  6578.50 ms\n",
      "llama_print_timings:      sample time =    41.34 ms /    58 runs   (    0.71 ms per token,  1403.14 tokens per second)\n",
      "llama_print_timings: prompt eval time = 18363.49 ms /   432 tokens (   42.51 ms per token,    23.52 tokens per second)\n",
      "llama_print_timings:        eval time =  2801.03 ms /    57 runs   (   49.14 ms per token,    20.35 tokens per second)\n",
      "llama_print_timings:       total time = 21368.38 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'```\\nThe issue is that the `model_path` parameter in the `LlamaCpp` constructor is not a valid file path. Check that the file path is correct and try again.\\nI hope this helps! Let me know if you have any other questions.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "I downloaded the 70b param llama model from HuggingFace. When trying to load in the model I get the following error. When loading the 7b model it works, help me debug the error.\n",
    "ValidationError                           Traceback (most recent call last)\n",
    "Cell In [6], line 2\n",
    "      1 # Make sure the model path is correct for your system!\n",
    "----> 2 llm = LlamaCpp(\n",
    "      3     model_path=\"/Users/vatsalpatel/Downloads/llama-2-70b-chat.ggmlv3.q4_0.bin\",\n",
    "      4     metadata={\"temperature\": 0, \"max_length\": 200000, \"top_p\": 1},\n",
    "      5     callback_manager=callback_manager,\n",
    "      6     verbose=True,\n",
    "      7 )\n",
    "\n",
    "File ~/opt/miniconda3/envs/test_env/lib/python3.10/site-packages/langchain/load/serializable.py:74, in Serializable.__init__(self, **kwargs)\n",
    "     73 def __init__(self, **kwargs: Any) -> None:\n",
    "---> 74     super().__init__(**kwargs)\n",
    "     75     self._lc_kwargs = kwargs\n",
    "\n",
    "File ~/opt/miniconda3/envs/test_env/lib/python3.10/site-packages/pydantic/main.py:341, in pydantic.main.BaseModel.__init__()\n",
    "\n",
    "ValidationError: 1 validation error for LlamaCpp\n",
    "__root__\n",
    "  Could not load Llama model from path: /Users/vatsalpatel/Downloads/llama-2-70b-chat.ggmlv3.q4_0.bin. Received error  (type=value_error)\n",
    "\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9cc590-9a74-49c7-8e23-05c9449d2016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89d9e1c6-2c12-4462-9393-8589f3b35051",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- https://ai.meta.com/blog/large-language-model-llama-meta-ai/ [1]\n",
    "- https://arxiv.org/abs/2302.13971 [2]\n",
    "- https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md [3]\n",
    "- https://pypi.org/project/langchain/ [4]\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
