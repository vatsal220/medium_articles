{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "168de318-3817-4a4e-92bf-6868424c3898",
   "metadata": {},
   "source": [
    "# Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b49769-f6f6-4fce-9c11-75d049198bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0149edf3-39ce-4acf-a6f0-a5ed481ca5e4",
   "metadata": {},
   "source": [
    "You can install the arxiv package in Python with the following command:   \n",
    "`pip install arxiv`   \n",
    "or follow the instructions here : https://pypi.org/project/arxiv/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd226a28-bf3a-4b14-80de-2193c6a851b2",
   "metadata": {},
   "source": [
    "## What is Node Classification?  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf4a040-67f8-421b-aef3-2ccbac7f1946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import arxiv\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, confusion_matrix, classification_report\n",
    "from node2vec import Node2Vec as n2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af08b4c-d203-4aa3-be20-2271b06d8827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "queries = [\n",
    "    'automl', 'machinelearning', 'data', 'phyiscs','mathematics', 'recommendation system', 'nlp', 'neural networks'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c79e656-b9a1-4c8a-bc5c-016e1ab8b23b",
   "metadata": {},
   "source": [
    "## Fetch Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab49a7c-266f-4a48-b9de-92bb8f0d5e75",
   "metadata": {},
   "source": [
    "We want to hit th Arxiv API to gather some information about the latest research papers based on the queries we've identified above. This will allow us to then create a network from this research paper data and then we can try to predict links on that network. For the purposes of this article, I will search for a maximum of 1000 results per query, but you don't have to set yourself to the same constraints. The Arxiv API allows users to hit up to 300,000 results per query. The function outlined below will generate a CSV fetching the following information :\n",
    "'title', 'date', 'article_id', 'url', 'main_topic', 'all_topics', 'authors', 'year'\n",
    "You are able to fetch more information like the links, summary, article but I decided not to since those features won't really be used for the purposes of this analysis and tutorial.\n",
    "\n",
    "For reference to the Arxiv API, you can find their detailed documentation here : https://arxiv.org/help/api/user-manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944f1ccb-69db-4c2f-a5a2-9361c5637d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv(queries, max_results = 100):\n",
    "    '''\n",
    "    This function will search arxiv associated to a set of queries and store\n",
    "    the latest 10000 (max_results) associated to that search.\n",
    "    \n",
    "    params:\n",
    "        queries (List -> Str) : A list of strings containing keywords you want\n",
    "                                to search on Arxiv\n",
    "        max_results (Int) : The maximum number of results you want to see associated\n",
    "                            to your search. Default value is 1000, capped at 300000\n",
    "                            \n",
    "    returns:\n",
    "        This function will return a DataFrame holding the following columns associated\n",
    "        to the queries the user has passed. \n",
    "            `title`, `date`, `article_id`, `url`, `main_topic`, `all_topics`\n",
    "    \n",
    "    example:\n",
    "        research_df = search_arxiv(\n",
    "            queries = ['automl', 'recommender system', 'nlp', 'data science'],\n",
    "            max_results = 10000\n",
    "        )\n",
    "    '''\n",
    "    d = []\n",
    "    searches = []\n",
    "    # hitting the API\n",
    "    for query in queries:\n",
    "        search = arxiv.Search(\n",
    "          query = query,\n",
    "          max_results = max_results,\n",
    "          sort_by = arxiv.SortCriterion.SubmittedDate,\n",
    "          sort_order = arxiv.SortOrder.Descending\n",
    "        )\n",
    "        searches.append(search)\n",
    "    \n",
    "    # Converting search result into df\n",
    "    for search in searches:\n",
    "        for res in search.results():\n",
    "            data = {\n",
    "                'title' : res.title,\n",
    "                'date' : res.published,\n",
    "                'article_id' : res.entry_id,\n",
    "                'url' : res.pdf_url,\n",
    "                'main_topic' : res.primary_category,\n",
    "                'all_topics' : res.categories,\n",
    "                'authors' : res.authors\n",
    "            }\n",
    "            d.append(data)\n",
    "        \n",
    "    d = pd.DataFrame(d)\n",
    "    d['year'] = pd.DatetimeIndex(d['date']).year\n",
    "    \n",
    "    # change article id from url to integer\n",
    "    unique_article_ids = d.article_id.unique()\n",
    "    article_mapping = {art:idx for idx,art in enumerate(unique_article_ids)}\n",
    "    d['article_id'] = d['article_id'].map(article_mapping)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a0f9af-524d-4370-bce7-f8c6855152dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "research_df = search_arxiv(\n",
    "    queries = queries,\n",
    "    max_results = 1000\n",
    ")\n",
    "research_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d9f4e-3e3f-4f80-9049-ca44751005bb",
   "metadata": {},
   "source": [
    "If you're having trouble querying the data, for reproducibility purposes, the CSV I used for the analysis conducted in this article was uploaded to my GitHub which you can find here. https://github.com/vatsal220/medium_articles/blob/main/link_prediction/data/arxiv_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7886d3-e135-495e-a96a-aa141cf51fae",
   "metadata": {},
   "source": [
    "## Generate Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aca878-fac8-4812-9c2b-f181efca75a4",
   "metadata": {},
   "source": [
    "Now that we've fetched the data using the Arxiv API, we can generate a network. The network will have the following structure, nodes will be the article_ids and the edges will be all topics connecting a pair of articles. For example, article_id 1 with the following topics astro-physics, and `stats` can be connected to article_id 10 with the topic stats and article_id 7 with the topics `astro-physics`, `math`. This will be a multi-edge network where each edge will hold a weight of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6e106a-ba56-4659-a188-772306d5c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network(df, node_col, edge_col):\n",
    "    '''\n",
    "    This function will generate a article to article network given an input DataFrame.\n",
    "    It will do so by creating an edge_dictionary where each key is going to be a node\n",
    "    referenced by unique values in node_col and the values will be a list of other nodes\n",
    "    connected to the key through the edge_col.\n",
    "    \n",
    "    params:\n",
    "        df (DataFrame) : The dataset which holds the node and edge columns\n",
    "        node_col (String) : The column name associated to the nodes of the network\n",
    "        edge_col (String) : The column name associated to the edges of the network\n",
    "        \n",
    "    returns:\n",
    "        A networkx graph corresponding to the input dataset\n",
    "        \n",
    "    example:\n",
    "        generate_network(\n",
    "            research_df,\n",
    "            node_col = 'article_id',\n",
    "            edge_col = 'main_topic'\n",
    "        )\n",
    "    '''\n",
    "    edge_dct = {}\n",
    "    for i,g in df.groupby(node_col):\n",
    "        topics = g[edge_col].unique()\n",
    "        edge_df = df[(df[node_col] != i) & (df[edge_col].isin(topics))]\n",
    "        edges = list(edge_df[node_col].unique())\n",
    "        edge_dct[i] = edges\n",
    "    \n",
    "    # create nx network\n",
    "    g = nx.Graph(edge_dct, create_using = nx.MultiGraph)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e739a71-2a1e-4dfd-aae1-620410bfca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tp_nx = generate_network(\n",
    "    research_df, \n",
    "    node_col = 'article_id', \n",
    "    edge_col = 'main_topic'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb39fb-b179-4108-8993-084b4156caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(tp_nx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e5a6ab-eca7-46f1-91b0-49e96514ecf6",
   "metadata": {},
   "source": [
    "## Apply Node2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4d822-2f96-426a-89dc-26e70d4eff50",
   "metadata": {},
   "source": [
    "This component will cover running node2vec on the graph generated above and creating the associated node embeddings for that network. These embeddings will play a crucial role coming up as they're the main features necessary for building a link prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23802c9a-b3a4-435b-b53d-4a01b88ac758",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time g_emb = n2v(tp_nx, dimensions=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff2d00-67dc-4a1b-9d09-b970f82088a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 1 # Node2Vec fit window\n",
    "MIN_COUNT = 1 # Node2Vec min. count\n",
    "BATCH_WORDS = 4 # Node2Vec batch words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5b5f3-4b58-4e42-b2ad-67e25b99364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = g_emb.fit(\n",
    "    window=WINDOW,\n",
    "    min_count=MIN_COUNT,\n",
    "    batch_words=BATCH_WORDS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2f2af-0938-4378-9304-c620faac9012",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = (\n",
    "    pd.DataFrame(\n",
    "        [mdl.wv.get_vector(str(n)) for n in tp_nx.nodes()],\n",
    "        index = tp_nx.nodes\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a54708-bc49-4079-9683-fab39fa704a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe5920-52f9-4895-8b92-02a6a90590aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = emb_df.merge(\n",
    "    research_df[['article_id', 'main_topic']].set_index('article_id'),\n",
    "    left_index = True,\n",
    "    right_index = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d694c8-c686-400c-ab63-67c35db1fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_cols = emb_df.drop(columns = ['main_topic']).columns.tolist()\n",
    "target_col = 'main_topic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd314423-d2c0-4651-b543-952a461f00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "x = emb_df[ft_cols].values\n",
    "y = emb_df[target_col].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, \n",
    "    y,\n",
    "    test_size = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f87fc5-a687-4a6a-a9db-6480161aed88",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6604426-599b-4775-9a02-67f42e5cb736",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# GBC classifier\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "# train the model\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e382b-f693-4d4f-85ac-5ec0fc696583",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d2504-5e41-4bbc-b6ea-28a4af5ef86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_eval(clf, x_test, y_test):\n",
    "    '''\n",
    "    This function will evaluate a sk-learn multi-class classification model based on its\n",
    "    x_test and y_test values\n",
    "    \n",
    "    params:\n",
    "        clf (Model) : The model you wish to evaluate the performance of\n",
    "        x_test (Array) : Result of the train test split\n",
    "        y_test (Array) : Result of the train test split\n",
    "    \n",
    "    returns:\n",
    "        This function will return the following evaluation metrics:\n",
    "            - Accuracy Score\n",
    "            - Matthews Correlation Coefficient\n",
    "            - Classification Report\n",
    "            - Confusion Matrix\n",
    "    \n",
    "    example:\n",
    "        clf_eval(\n",
    "            clf,\n",
    "            x_test,\n",
    "            y_test\n",
    "        )\n",
    "    '''\n",
    "    y_pred = clf.predict(x_test)\n",
    "    y_true = y_test\n",
    "    \n",
    "    y_pred = clf.predict(x_test)\n",
    "    x_pred = clf.predict(x_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"Testing Accuracy : \", test_acc)\n",
    "    \n",
    "    print(\"MCC Score : \", matthews_corrcoef(y_true, y_pred))\n",
    "    \n",
    "    print(\"Classification Report : \")\n",
    "    print(classification_report(y_test, clf.predict(x_test)))\n",
    "    \n",
    "    print(confusion_matrix(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b159b03e-ccf6-45ad-b622-e16eea1413de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf_eval(\n",
    "    clf,\n",
    "    x_test,\n",
    "    y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f438fe8b-ebbb-484d-bf34-716e4b7d9fd1",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ccbad5-9504-48f1-a780-7c2ec6a7b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ft = [mdl.wv.get_vector(str('21'))]\n",
    "clf.predict(pred_ft)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc1861e-d44b-48ab-abda-ddc3fc16b777",
   "metadata": {},
   "source": [
    "## Concluding Remarks\n",
    "\n",
    "\n",
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12205ff3-06b6-417f-9b65-52cf368f2c22",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
