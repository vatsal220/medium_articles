{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97fc3c50-5f36-43e0-bd46-fd0db55b47fa",
   "metadata": {},
   "source": [
    "# Efficient Model Training & Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185d5323-742a-47d0-8cb6-a4a47621bb38",
   "metadata": {},
   "source": [
    "Problem Outline: Common problem when dealing with large datasets and working in python is a matter of efficiency in both time and space complexity. Usual solutions to combat these problems are to either increase the computing power available for the computer (which can be expense) or to continue your pipeline in a distributive framework like Spark / Pyspark (which can be a hastle & exepnsive to set up). These sort of problems comes up often in industry settings, especially when working on problems which require you to aggregate a large amount of user data, problems associated to clustering / recommendation systems. Feeding all this data into inefficient models can be cumberson to deal with since the computer will most likely run out of memory. \n",
    "\n",
    "Problem Solution:\n",
    "We can solve this problem by optimizing the way we train and predict with our models, instead of passing in dense vectors for training & predicting, we pass in sparse vectors. As you will see below that this will drastically reduce both the speed and memory used for training and predicting with the model. \n",
    "\n",
    "Dense Matrix vs Sparse Matrix:\n",
    "\n",
    "- batch prediction\n",
    "- real time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ecfc7d-39de-4279-8f8f-2554afee0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import uuid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0034a9-1ff3-4326-8731-4094047fcf64",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dadb441c-91ef-4039-a2a2-ed7af06388e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(id_n, prd_n):\n",
    "    '''\n",
    "    This function will generate a dataframe with a lot of sparse values \n",
    "    \n",
    "    params:\n",
    "        id_n (Integer) : The number of user's you want in the DF\n",
    "        prd_n (Integer) : The number of products\n",
    "    \n",
    "    returns:\n",
    "        A dataframe with prd_n + 1 columns where majority of the values\n",
    "        are 0\n",
    "    \n",
    "    example:\n",
    "        df = generate_data(\n",
    "            id_n = 10000,\n",
    "            prd_n = 1000\n",
    "        )\n",
    "    '''\n",
    "    \n",
    "    dct = {\n",
    "        'user': [uuid.uuid4() for _ in range(id_n)], \n",
    "        'product': [random.randint(1, prd_n) for _ in range(id_n)],\n",
    "        'value' : [random.randint(1, 100) for N in range(id_n)]\n",
    "    }\n",
    "    d = pd.DataFrame(dct)\n",
    "    \n",
    "    # convert to a pivot table, replace the nan's with 0's\n",
    "    df = d.pivot_table(\n",
    "        values = 'value', index = 'user', columns = 'product'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    categories = ['category1', 'category2', 'category3', 'category4', 'category5']\n",
    "    df['category'] = [random.choice(categories) for _ in range(df.shape[0])]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6042254c-25f5-4cd8-a1b0-38acb9c6ed44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.11 s, sys: 2.11 s, total: 5.22 s\n",
      "Wall time: 6.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "df = generate_data(\n",
    "    id_n = 100000,\n",
    "    prd_n = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6de6a8d-bfb6-4a2c-bdd2-7a0912256bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 1001)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c8af3d9-bcc6-4877-bf91-8b1378500632",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_cols = [x for x in df.columns if x != 'category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38718d16-42cb-4e3a-b899-6944f3ebe463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 168 ms, sys: 375 ms, total: 542 ms\n",
      "Wall time: 984 ms\n"
     ]
    }
   ],
   "source": [
    "%time dense_vectors = df[ft_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2cd86a1-8215-4ac4-b9b3-53dce47a65ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.57 s, sys: 924 ms, total: 2.5 s\n",
      "Wall time: 3.3 s\n"
     ]
    }
   ],
   "source": [
    "%time sparse_vectors = scipy.sparse.csr_matrix(df[ft_cols].astype(float).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa19af7-0e45-454f-bf60-616acbf2aae9",
   "metadata": {},
   "source": [
    "## SVC - Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c910eab1-5c41-4513-b3a5-5abf1c53803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dense_vectors\n",
    "y = df['category'].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(X ,y ,test_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cad8466-dd2b-44e5-8cfd-035d2f7eebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dense_svc = SVC()\n",
    "dense_svc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2acd23-df08-407a-81c6-3cd7f6373a5e",
   "metadata": {},
   "source": [
    "## SVC - Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "155a2a2f-131b-4e2d-b3e1-f0c236e2e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sparse_vectors\n",
    "y = df['category'].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(X ,y ,test_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e49a8a94-348c-4175-91d9-f545216eaa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.5 s, sys: 1.17 s, total: 25.6 s\n",
      "Wall time: 27 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sparse_svc = SVC()\n",
    "sparse_svc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2208ea-1b48-4684-a04a-5f368d588ec7",
   "metadata": {},
   "source": [
    "## Predict SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e14b66-2426-46bb-bd4a-f1b3793b2e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9f033-09f2-4512-9247-049c239bbad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce79cffe-35bb-4e97-8f91-094e6aca2fd4",
   "metadata": {},
   "source": [
    "As you might've noticed, this solution most likely won't have a large impact on your application if you are not working with a large & sparse dataset. When the dataset doesn't have a lot of zeros in it, converting it to a sparse matrix and running the calculations won't have the same impact (if any at all). When this is the case, my advice would be to try paralellizing your code to run on multiple pools / threads. This will aid in reducing the amount of time associated to generating predictions & training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fecb0cb-5eb6-4ba8-ae66-45f1a24a34a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696644db-0701-4ac0-8476-d8b0eb06943a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
