{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d61c82-c84d-451b-846d-396b6857f0c4",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90420df-54b9-47be-a8ef-ee62517269c8",
   "metadata": {},
   "source": [
    "This article will provide an in depth overview of the mathematics behind linear and bayesian linear regression and their associated implementations in Python. The implementations will include building the algorithms from scratch as well as using Sci-Kit Learn (sklearn) to reference their pre-built linear regression model. \n",
    "\n",
    "### Table of Contents :   \n",
    "- Introduction to Linear Regression  \n",
    "    - Requirements of Linear Regression\n",
    "    - Maximum Likelihood Estimation  \n",
    "    - Overfitting\n",
    "    - Linear Regression Implementation  \n",
    "        - Requirements  \n",
    "    - Sci-Kit Learn Implementation\n",
    "- Introduction to Bayesian Linear Regression  \n",
    "    - Prior Predictions\n",
    "    - Posterior Distributions\n",
    "    - Marginal Likelihood\n",
    "    - Bayesian Linear Regression Implementation  \n",
    "        - Requirements  \n",
    "- Concluding Remarks  \n",
    "- Resources  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453928c7-ef7e-4603-b1a0-774c220a955b",
   "metadata": {},
   "source": [
    "## Introduction to Linear Regression\n",
    "Linear regression is a supervised learning method in machine learning which aims to model the relationship between 1 or more variables. Simple linear regression refers to the exploration between 1 dependent and 1 independent variable, whereas multiple linear regression explores the relationship between 2 or more independent variables and 1 dependent variable. There are many real world applications of linear regression, it is commonly used in time series analysis, optimization, ranking, reinforcement learning, etc. The overall intuition behind linear regression is to fit a line to some dataset, calculate the R^2 and calculate the p-value for the R^2. \n",
    "\n",
    "![Figure 1 : Basic Linear Regression](./img/intro.png)\n",
    "\n",
    "The image above outlines the following, first create a line of best fit across the dataset. Calculate the residual squared (the distance between the observation to the line) and repeat the process while slightly rotating the line each time. You can map the sum of the residuals to the corresponding line of best fit, the line with the lowest residual sum is the best line to model that dataset.\n",
    "\n",
    "## Requirements of Linear Regression\n",
    "There are 4 major assumptions of linear regression. These assumptions correspond to the relationship between the input X and Y variables, all 4 of these assumptions must be valid, otherwise the results of the model will be misleading / unreliable.  \n",
    "**Note:** Residuals refer to the difference between an observed value of the response variable and the value of the response variable predicted from the regression line [2].    \n",
    "1) Linear Relationship  \n",
    "    - There exists a linear relationship between the X and Y variables. You can easily validate this assumption through generating a scatter plot. If the resulting graph of your X vs Y variable looks linear, then this assumption is valid. \n",
    "2) Independence  \n",
    "    - Residuals must be independent. The independence of residuals refers to when values at time step i+1 are not independent from values at time step i. You can check the independence of residuals through plotting residuals against time variables. A non random pattern implies that there is a lack of independence.\n",
    "3) Homoscedasticity  \n",
    "    - The variance of the errors must not depend on the values of the predictors. This implies that the responses for fixed values of predictors is the same regardless of how large or small the responses are [3].  \n",
    "4) Normality  \n",
    "    - The residuals are normally distributed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3017f5a-8300-4258-a877-58606d6155f7",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "Given a numerical dataset consisting of dependent and independent variable(s) to train the model, we can use maximum likelihood estimation (MLE) for estimating the parameters of a probability distribution. You can interpret this as maximizing the predictive distribution of the training data given the parameters [1]. This is the method we're going to use to fit a line to a given dataset. \n",
    "\n",
    "Note : Without going too deep into the mathematics of it, the method of maximum likelihood and the method of ordinary least squares will return the same values of parameters estimated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e468ca78-ceb1-44e9-9987-a8eb20d4ad7a",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "Overfitting in machine learning refers to when a model performs very highly on the dataset it was trained on (observed data) but performs poorly on a dataset it has not yet seen before. This indicates that the model has been overfit to the training data, implying any observations not similar to the initial training set, would yield poor predictions. \n",
    "\n",
    "Often to calculate the performance of a regression model, RMSE is used. The formula below [4] outlines how RMSE is calculated :   \n",
    "![Figure 1 : RMSE](./img/rmse.png)\n",
    "\n",
    "Here are some recommendations on how to stop a model from overfitting : \n",
    "1) Hold out set  \n",
    "    - Using a train test split (70/30 or 80/20)  \n",
    "2) Cross validation  \n",
    "    - We can split our dataset into k groups (k-fold cross-validation). We let one of the groups be the testing set (please see hold-out explanation) and the others as the training set, and repeat this process until each individual group has been used as the testing set (e.g., k repeats). This process is computationally expensive.  \n",
    "3) Increasing the training data  \n",
    "    - Introducing new data which the model has not seen before  \n",
    "4) Feature selection  \n",
    "    - Selecting the most important features and reducing the noise you're feeding into the model  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77174029-ccf1-480e-8bc2-575693f91956",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98fb1df-e615-4e84-ab56-65a395145c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407653b-5488-4cde-9f94-10de2e0075ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e40b82b3-d427-4ecc-a52a-b0e330b420ce",
   "metadata": {},
   "source": [
    "# Bayesian Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd53784d-6a02-4363-b689-22572c0da1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff06a6b-5e3e-4c2e-8b85-4168850417ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ee66b36-e043-4de6-b7fb-1a120534c701",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- [1] Deisenroth, Marc Peter, et al. Mathematics for Machine Learning. Cambridge University Press, 2020. https://mml-book.github.io/\n",
    "- [2] https://nzmaths.co.nz/category/glossary/residual-linear-regression\n",
    "- [3] https://en.wikipedia.org/wiki/Linear_regression\n",
    "- [4] https://www.statisticshowto.com/probability-and-statistics/regression-analysis/rmse-root-mean-square-error/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee73b5-7b3b-4fb1-9377-5cb082acb41e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
