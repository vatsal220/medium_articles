{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09fc26ff-d7e5-4b7f-8de5-e443fb176bee",
   "metadata": {},
   "source": [
    "# PyTorch Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ef8248-7a57-4439-a165-b6c3b5fe0f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Requirement:\n",
    "    !pip install torch-cluster==latest+cu101 torch-scatter==latest+cu101 torch-sparse==latest+cu101 -f https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.7.0.html\n",
    "    !pip install torch-geometric\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aeeb5f-8369-4409-9930-f517da0c8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.nn import Embedding\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_sparse import SparseTensor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "try:\n",
    "    import torch_cluster  # noqa\n",
    "    random_walk = torch.ops.torch_cluster.random_walk\n",
    "except ImportError:\n",
    "    random_walk = None\n",
    "EPS = 1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ba97da-2f51-40ca-bc13-8f8126b39a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node2Vec(torch.nn.Module):\n",
    "    r\"\"\"The Node2Vec model from the\n",
    "    `\"node2vec: Scalable Feature Learning for Networks\"\n",
    "    <https://arxiv.org/abs/1607.00653>`_ paper where random walks of\n",
    "    length :obj:`walk_length` are sampled in a given graph, and node embeddings\n",
    "    are learned via negative sampling optimization.\n",
    "    .. note::\n",
    "        For an example of using Node2Vec, see `examples/node2vec.py\n",
    "        <https://github.com/rusty1s/pytorch_geometric/blob/master/examples/\n",
    "        node2vec.py>`_.\n",
    "    Args:\n",
    "        edge_index (LongTensor): The edge indices.\n",
    "        embedding_dim (int): The size of each embedding vector.\n",
    "        walk_length (int): The walk length.\n",
    "        context_size (int): The actual context size which is considered for\n",
    "            positive samples. This parameter increases the effective sampling\n",
    "            rate by reusing samples across different source nodes.\n",
    "        walks_per_node (int, optional): The number of walks to sample for each\n",
    "            node. (default: :obj:`1`)\n",
    "        p (float, optional): Likelihood of immediately revisiting a node in the\n",
    "            walk. (default: :obj:`1`)\n",
    "        q (float, optional): Control parameter to interpolate between\n",
    "            breadth-first strategy and depth-first strategy (default: :obj:`1`)\n",
    "        num_negative_samples (int, optional): The number of negative samples to\n",
    "            use for each positive sample. (default: :obj:`1`)\n",
    "        num_nodes (int, optional): The number of nodes. (default: :obj:`None`)\n",
    "        sparse (bool, optional): If set to :obj:`True`, gradients w.r.t. to the\n",
    "            weight matrix will be sparse. (default: :obj:`False`)\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_index, embedding_dim, walk_length, context_size,\n",
    "                 walks_per_node=1, p=1, q=1, num_negative_samples=1,\n",
    "                 num_nodes=None, sparse=False):\n",
    "        super(Node2Vec, self).__init__()\n",
    "        if random_walk is None:\n",
    "            raise ImportError('`Node2Vec` requires `torch-cluster`.')\n",
    "        N = maybe_num_nodes(edge_index, num_nodes)\n",
    "        row, col = edge_index\n",
    "        self.adj = SparseTensor(row=row, col=col, sparse_sizes=(N, N))\n",
    "        self.adj = self.adj.to('cpu')\n",
    "        assert walk_length >= context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.walk_length = walk_length - 1\n",
    "        self.context_size = context_size\n",
    "        self.walks_per_node = walks_per_node\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "        self.embedding = Embedding(N, embedding_dim, sparse=sparse)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.embedding.reset_parameters()\n",
    "        \n",
    "    def forward(self, batch=None):\n",
    "        \"\"\"Returns the embeddings for the nodes in :obj:`batch`.\"\"\"\n",
    "        emb = self.embedding.weight\n",
    "        return emb if batch is None else emb[batch]\n",
    "    \n",
    "    def loader(self, **kwargs):\n",
    "        return DataLoader(range(self.adj.sparse_size(0)),\n",
    "                          collate_fn=self.sample, **kwargs)\n",
    "    \n",
    "    def pos_sample(self, batch):\n",
    "        batch = batch.repeat(self.walks_per_node)\n",
    "        rowptr, col, _ = self.adj.csr()\n",
    "        rw = random_walk(rowptr, col, batch, self.walk_length, self.p, self.q)\n",
    "        if not isinstance(rw, torch.Tensor):\n",
    "            rw = rw[0]\n",
    "        walks = []\n",
    "        num_walks_per_rw = 1 + self.walk_length + 1 - self.context_size\n",
    "        for j in range(num_walks_per_rw):\n",
    "            walks.append(rw[:, j:j + self.context_size])\n",
    "        return torch.cat(walks, dim=0)\n",
    "    \n",
    "    def neg_sample(self, batch):\n",
    "        batch = batch.repeat(self.walks_per_node * self.num_negative_samples)\n",
    "        rw = torch.randint(self.adj.sparse_size(0),\n",
    "                           (batch.size(0), self.walk_length))\n",
    "        rw = torch.cat([batch.view(-1, 1), rw], dim=-1)\n",
    "        walks = []\n",
    "        num_walks_per_rw = 1 + self.walk_length + 1 - self.context_size\n",
    "        for j in range(num_walks_per_rw):\n",
    "            walks.append(rw[:, j:j + self.context_size])\n",
    "        return torch.cat(walks, dim=0)\n",
    "    \n",
    "    def sample(self, batch):\n",
    "        if not isinstance(batch, torch.Tensor):\n",
    "            batch = torch.tensor(batch)\n",
    "        return self.pos_sample(batch), self.neg_sample(batch)\n",
    "    \n",
    "    def loss(self, pos_rw, neg_rw):\n",
    "        r\"\"\"Computes the loss given positive and negative random walks.\"\"\"\n",
    "        # Positive loss.\n",
    "        start, rest = pos_rw[:, 0], pos_rw[:, 1:].contiguous()\n",
    "        h_start = self.embedding(start).view(pos_rw.size(0), 1,\n",
    "                                             self.embedding_dim)\n",
    "        h_rest = self.embedding(rest.view(-1)).view(pos_rw.size(0), -1,\n",
    "                                                    self.embedding_dim)\n",
    "        out = (h_start * h_rest).sum(dim=-1).view(-1)\n",
    "        pos_loss = -torch.log(torch.sigmoid(out) + EPS).mean()\n",
    "        # Negative loss.\n",
    "        start, rest = neg_rw[:, 0], neg_rw[:, 1:].contiguous()\n",
    "        h_start = self.embedding(start).view(neg_rw.size(0), 1,\n",
    "                                             self.embedding_dim)\n",
    "        h_rest = self.embedding(rest.view(-1)).view(neg_rw.size(0), -1,\n",
    "                                                    self.embedding_dim)\n",
    "        out = (h_start * h_rest).sum(dim=-1).view(-1)\n",
    "        neg_loss = -torch.log(1 - torch.sigmoid(out) + EPS).mean()\n",
    "        return pos_loss + neg_loss\n",
    "    \n",
    "    def test(self, train_z, train_y, test_z, test_y, solver='lbfgs',\n",
    "             multi_class='auto', *args, **kwargs):\n",
    "        r\"\"\"Evaluates latent space quality via a logistic regression downstream\n",
    "        task.\"\"\"\n",
    "        clf = LogisticRegression(solver=solver, multi_class=multi_class, *args,\n",
    "                                 **kwargs).fit(train_z.detach().cpu().numpy(),\n",
    "                                               train_y.detach().cpu().numpy())\n",
    "        return clf.score(test_z.detach().cpu().numpy(), test_y.detach().cpu().numpy())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__,\n",
    "                                   self.embedding.weight.size(0),\n",
    "                                   self.embedding.weight.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4f3e27-35ef-4183-9cb6-342d4a2744ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import construct_graph_data\n",
    "path = os.getcwd()\n",
    "code_dir = os.path.join(path, 'src/')\n",
    "data_dir = os.path.join(path, 'data/')\n",
    "citation_go, _, node2patent = construct_graph_data()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Node2Vec(edge_index=citation_go.edge_index, \n",
    "                 embedding_dim=128, \n",
    "                 walk_length=20,\n",
    "                 context_size=10, \n",
    "                 walks_per_node=10, \n",
    "                 num_negative_samples=1,\n",
    "                 p=1, q=1, \n",
    "                 sparse=True).to(device)\n",
    "loader = model.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a3f862-a214-41c1-a2cc-2d9394e39f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    z = model()\n",
    "    acc = model.test(z[citation_go.train_mask], citation_go.y[citation_go.train_mask],\n",
    "                     z[citation_go.test_mask], citation_go.y[citation_go.test_mask], max_iter=150)\n",
    "    return acc\n",
    "\n",
    "for epoch in range(1, 50):\n",
    "    loss = train()\n",
    "    # acc = test()\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')#, Acc: {acc:.4f}')\n",
    "a = model.embedding.weight.detach().cpu().numpy()\n",
    "df_embeding = pd.DataFrame(a)\n",
    "nb_node = df_embeding.shape[0]\n",
    "list_patent_index = [node2patent[i] for i in range(nb_node)]\n",
    "df_embeding.index=list_patent_index\n",
    "df_embeding.to_csv(\"node2vec_embedding.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752d517-1eb6-43ce-a96e-498b8d692f00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
