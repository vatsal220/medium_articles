{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b6a9ed-5eea-4661-bdfe-b554faabfa65",
   "metadata": {},
   "source": [
    "# Twitter - Crime Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776921b0-f7f2-4e99-938c-e5b2dcce694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "# from utils.db_utils import *\n",
    "# from utils.twitter_utils import *\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4108740e-6507-4946-8a40-c3420167acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "PATH = os.path.expanduser('~') + '/'\n",
    "env_file = '.env'\n",
    "today = datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec19700d-f0e1-4592-bd6a-287ee7a0995b",
   "metadata": {},
   "source": [
    "## Connect to Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5fe4bb1-fbb9-4d5b-8522-ab540f615c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "def env_reader(env_path):\n",
    "    '''\n",
    "    This function will read the environment variables given the location of where the\n",
    "    .env file is located\n",
    "    '''\n",
    "    if not load_dotenv(env_path):\n",
    "        return print('Environment File Not Found')\n",
    "    return print('Loaded!')\n",
    "\n",
    "env_reader(os.getcwd() + '/' + env_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "403d8641-957b-4d89-bfd3-ce4eefe168ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to Twitter established.\n"
     ]
    }
   ],
   "source": [
    "def connect_twitter(api_key, secret_key, access_token, secret_access_token):\n",
    "    '''\n",
    "    This function will create a connection to the twitter api using the necessary\n",
    "    credentials associated to the project.\n",
    "    \n",
    "    params:\n",
    "        api_key (String) : Taken from twitter developer account\n",
    "        secret_key (String) : Taken from twitter developer account\n",
    "        access_token (String) : Taken from twitter developer account\n",
    "        secret_access_token (String) : Taken from twitter developer account\n",
    "         \n",
    "    returns:\n",
    "        This function will return an API which can be called\n",
    "    '''\n",
    "    auth = tp.OAuthHandler(\n",
    "        consumer_key = api_key, \n",
    "        consumer_secret = secret_key\n",
    "    )\n",
    "    auth.set_access_token(\n",
    "        key =  access_token, \n",
    "        secret = secret_access_token\n",
    "    )\n",
    "    api = tp.API(auth)\n",
    "    \n",
    "    try:\n",
    "        api.verify_credentials()\n",
    "        print(\"Connection to Twitter established.\")\n",
    "    except:\n",
    "        print(\"Failed to connect to Twitter.\")\n",
    "    return api\n",
    "\n",
    "api = connect_twitter(\n",
    "    api_key = os.getenv(\"twitter_api_key\"), \n",
    "    secret_key = os.getenv(\"twitter_secret\"),\n",
    "    access_token = os.getenv(\"twitter_access_token\"),\n",
    "    secret_access_token = os.getenv(\"twitter_access_token_secret\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d301263-8cec-44fa-8875-46e4a8b02991",
   "metadata": {},
   "source": [
    "## Get Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c258b0f-2cc8-456b-98c8-48a5cd57d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_dir(path, date):\n",
    "    '''\n",
    "    The purpose of this function is to make a directory if one does not exist of the current\n",
    "    date in the data folder.\n",
    "    \n",
    "    params:\n",
    "        path (String) : The path to the data folder\n",
    "        date (String) : The current date yyyy-mm-dd\n",
    "        \n",
    "    returns:\n",
    "        This function will do nothing if the folder already exists, otherwise it will create\n",
    "        the folder\n",
    "    \n",
    "    example:\n",
    "        mk_dir(path = './data/', date = '2021-09-23')\n",
    "    '''\n",
    "    \n",
    "    # check if directory exists \n",
    "    exists = os.path.exists(path + date)\n",
    "    \n",
    "    if not exists:\n",
    "        os.makedirs(path + date)\n",
    "        print(\"New Directory Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27bc6ef6-3cce-466c-bdcf-4562ae6591d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tweets(screen_name, api = api, today = today):\n",
    "    '''\n",
    "    This function will get the latest ~3200 tweets associated to a twitter screen name.\n",
    "    It will proceed to get the tweet id, created at and the content and store it in a df.\n",
    "    It will save the associated results in a CSV file.\n",
    "    \n",
    "    params:\n",
    "        screen_name (String) : The twitter handle associated to the user you want to get\n",
    "                               tweets from\n",
    "        api (API) : The tweepy API connection\n",
    "        today (String) : Todays date in string format\n",
    "    \n",
    "    returns:\n",
    "        This function will return a df associated to the tweet id, created_at and content\n",
    "        \n",
    "    source:\n",
    "        [yanofsky](https://gist.github.com/yanofsky/5436496)\n",
    "    '''\n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []  \n",
    "    \n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
    "    \n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "    \n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "    \n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "#         print(f\"getting tweets before {oldest}\")\n",
    "        \n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
    "        \n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "        \n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "        \n",
    "#         print(f\"...{len(alltweets)} tweets downloaded so far\")\n",
    "    \n",
    "    #transform the tweepy tweets into a 2D array that will populate the csv \n",
    "    outtweets = [\n",
    "        [\n",
    "            t.author.name, t.id_str, t.created_at, t.text, t.entities.get('hashtags'), t.author.location,\n",
    "            t.author.created_at, t.author.url, t.author.screen_name, t.favorite_count, t.favorited,\n",
    "            t.retweet_count, t.retweeted, t.author.followers_count, t.author.friends_count\n",
    "        ] for t in alltweets\n",
    "    ]\n",
    "    \n",
    "    #write the csv  \n",
    "    cols = [\n",
    "        'author_name', 'tweet_id', 'tweet_created_at', 'content', 'hashtags', 'location',\n",
    "        'author_created_at', 'author_url', 'author_screen_name', 'tweet_favourite_count', 'tweet_favourited', \n",
    "        'retweet_count', 'retweeted', 'author_followers_count', 'author_friends_count'\n",
    "    ]\n",
    "    df = pd.DataFrame(outtweets, columns = cols)\n",
    "    mk_dir(path = './data/', date = today)\n",
    "    df.to_csv('./data/{}/{}_tweets_{}.csv'.format(today, screen_name, today), index = False)\n",
    "    time.sleep(10)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e803b37c-a6d0-4ff4-afc7-f391ed906586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweet_data(path):\n",
    "    '''\n",
    "    This function will identify if todays data has already been scraped from the twitter API.\n",
    "        - If it has been scraped, this function will read all the scraped data from today \n",
    "           and previous days\n",
    "    Upon fetching all the data, it will drop duplicates on the tweet_id and tweet_created_at\n",
    "    columns to remove duplicated tweets scraped from previous days.\n",
    "    \n",
    "    params:\n",
    "        path (String) : The path to the data folder\n",
    "        today (String) : Today's date in string format yyyy-mm-dd\n",
    "        \n",
    "    returns:\n",
    "        This function will return the tweets_df associated to tweets from all handles over \n",
    "        the past few months\n",
    "        \n",
    "    example:\n",
    "        read_tweet_data(\n",
    "            path = './data/'\n",
    "        )\n",
    "    '''\n",
    "    \n",
    "    # get all non hidden subdirectories from the path\n",
    "    sub_dir = [d for d in os.listdir(path) if d[0] != '.']\n",
    "    \n",
    "    # read csv from all sub directories and concat results\n",
    "    files = []\n",
    "    for d in sub_dir:\n",
    "        for p in os.listdir(path + d):\n",
    "            if p[0] != '.':\n",
    "                files.append(path + d + '/' + p)\n",
    "\n",
    "    read_csvs = []\n",
    "    for file in files:\n",
    "        read_csvs.append(pd.read_csv(file, converters={'hashtags': eval}, encoding='utf-8-sig'))\n",
    "\n",
    "    read_csvs = pd.concat(read_csvs)\n",
    "    tweets_df = read_csvs.drop_duplicates(subset = ['tweet_id', 'tweet_created_at'])\n",
    "    return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "488f1ecc-c671-4db1-a7d5-7f493f3fcf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = [\n",
    "    'CP24', 'TPSOperations', 'TorontoPolice'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc882469-248c-40be-a673-c298845d85bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP24\n",
      "New Directory Created\n",
      "TPSOperations\n",
      "TorontoPolice\n",
      "CPU times: user 4.27 s, sys: 623 ms, total: 4.89 s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if today not in os.listdir('./data/'):\n",
    "    for user in handles:\n",
    "        print(user)\n",
    "        _ = get_all_tweets(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9b6a988-210c-4261-9edc-319bd8291953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 87.2 ms, sys: 13.2 ms, total: 100 ms\n",
      "Wall time: 135 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tweets_df = read_tweet_data(\n",
    "    path = './data/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fabd3e1-ff3a-4f10-ba71-c341bfb3f9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9749, 15)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ebe55-b0ce-4b61-aeef-006d933bad8a",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6325112e-cbf5-41ef-a6fe-cc3b29f954f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_hashtags(row):\n",
    "    if row['hashtags_count'] > 1:\n",
    "        tags = row['hashtags']\n",
    "        hashies = []\n",
    "        for t in tags:\n",
    "            hashies.append(t['text'])\n",
    "        return hashies\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00242c26-0f4e-4d33-9b32-fb7bb27b4466",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['hashtags_count'] = tweets_df['hashtags'].apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb8b4735-f1f4-4e5e-90d4-400efa0fe29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.7 ms, sys: 15.5 ms, total: 75.2 ms\n",
      "Wall time: 82.1 ms\n"
     ]
    }
   ],
   "source": [
    "%time tweets_df['hashtags'] = tweets_df.apply(lambda x : parse_hashtags(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07c57286-05be-4a6a-a509-f2e6bdbdfad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-01-15 21:24:00+00:00'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[tweets_df['author_name'] == 'Toronto Police'].tweet_created_at.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07fddc1e-5818-4246-bd4a-c17fa6105e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519d939-4e61-4310-9fcc-f12fb7b2e869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed91f0d1-ebf5-4237-aba4-fa7f0c6e46a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59bfe8b-4d11-435a-9868-3a5cb2db8f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
