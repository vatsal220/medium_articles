{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the text file containing the training data\n",
    "training_data_file = 'eminem_songs_lyrics.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(line):\n",
    "    '''\n",
    "    Given a string, this function will remove the punctuation associated to it\n",
    "    \n",
    "    args:\n",
    "        line (String) : The sentence you want to remomve punctuations from\n",
    "    \n",
    "    example:\n",
    "        line = 'bonjour! hello world@~ whatt a time to, be alive~!@#$%^&*'\n",
    "        remove_punctuation(line)\n",
    "        >> 'bonjour hello world whatt a time to be alive'\n",
    "    '''\n",
    "    return line.translate(str.maketrans('','', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add2dict(dictionary, key, value):\n",
    "    '''\n",
    "    If the key is not present in the dictionary, it will create a value of an empty list associated to the key. If the key is present in the dicitonary\n",
    "    this function will append the value associated to that key \n",
    "    '''\n",
    "    if key not in dictionary:\n",
    "        dictionary[key] = []\n",
    "    dictionary[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2probabilitydict(given_list):\n",
    "    '''\n",
    "    Takes a list of words and creates a dictionary of probabilities associated to the words in the list\n",
    "    '''\n",
    "    probability_dict = {}\n",
    "    given_list_length = len(given_list)\n",
    "    for item in given_list:\n",
    "        probability_dict[item] = probability_dict.get(item, 0) + 1\n",
    "    for key, value in probability_dict.items():\n",
    "        probability_dict[key] = value / given_list_length\n",
    "    return probability_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_word = {}\n",
    "second_word = {}\n",
    "transitions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains a Markov model based on the data in training_data_file\n",
    "def train_markov_model():\n",
    "    for line in open(training_data_file):\n",
    "        tokens = remove_punctuation(line.rstrip().lower()).split()\n",
    "        tokens_length = len(tokens)\n",
    "        for i in range(tokens_length):\n",
    "            token = tokens[i]\n",
    "            if i == 0:\n",
    "                initial_word[token] = initial_word.get(token, 0) + 1\n",
    "            else:\n",
    "                prev_token = tokens[i - 1]\n",
    "                if i == tokens_length - 1:\n",
    "                    add2dict(transitions, (prev_token, token), 'END')\n",
    "                if i == 1:\n",
    "                    add2dict(second_word, prev_token, token)\n",
    "                else:\n",
    "                    prev_prev_token = tokens[i - 2]\n",
    "                    add2dict(transitions, (prev_prev_token, prev_token), token)\n",
    "    \n",
    "    # Normalize the distributions\n",
    "    initial_word_total = sum(initial_word.values())\n",
    "    for key, value in initial_word.items():\n",
    "        initial_word[key] = value / initial_word_total\n",
    "        \n",
    "    for prev_word, next_word_list in second_word.items():\n",
    "        second_word[prev_word] = list2probabilitydict(next_word_list)\n",
    "        \n",
    "    for word_pair, next_word_list in transitions.items():\n",
    "        transitions[word_pair] = list2probabilitydict(next_word_list)\n",
    "    \n",
    "    print('Training successful.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training successful.\n"
     ]
    }
   ],
   "source": [
    "train_markov_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word(dictionary):\n",
    "    p0 = np.random.random()\n",
    "    cumulative = 0\n",
    "    for key, value in dictionary.items():\n",
    "        cumulative += value\n",
    "        if p0 < cumulative:\n",
    "            return key\n",
    "    assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate sample text\n",
    "def generate(number_of_sentences):\n",
    "    for i in range(number_of_sentences):\n",
    "        sentence = []\n",
    "        # Initial word\n",
    "        word0 = sample_word(initial_word)\n",
    "        sentence.append(word0)\n",
    "        # Second word\n",
    "        word1 = sample_word(second_word[word0])\n",
    "        sentence.append(word1)\n",
    "        # Subsequent words untill END\n",
    "        while True:\n",
    "            word2 = sample_word(transitions[(word0, word1)])\n",
    "            if word2 == 'END':\n",
    "                break\n",
    "            sentence.append(word2)\n",
    "            word0 = word1\n",
    "            word1 = word2\n",
    "        print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enough rhymes to\n",
      "what else can i say if life was a highway\n",
      "but as rude and as indecent as all hell\n",
      "to meet rundmc and induct them\n",
      "its not hiphop and iâ€™m just not eminem\n"
     ]
    }
   ],
   "source": [
    "generate(number_of_sentences = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Markov():\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        \n",
    "        self.text = self.remove_punctuations(self.get_text())\n",
    "        self.model = self.model()\n",
    "        \n",
    "    def get_text(self):\n",
    "        '''\n",
    "        This function will read the input file and return the text associated to the file line by line in a list\n",
    "        '''\n",
    "        text = []\n",
    "        for line in open(self.file_path):\n",
    "            text.append(line)\n",
    "        return ' '.join(text)\n",
    "    \n",
    "    def remove_punctuations(self, text):\n",
    "        '''\n",
    "        Given a string of text this function will return the same input text without any punctuations\n",
    "        '''\n",
    "        return text.translate(str.maketrans('','', string.punctuation))\n",
    "    \n",
    "    def model(self):\n",
    "        '''\n",
    "        This function will take a block of text as the input and map each word in the text to a key where the\n",
    "        values associated to that key are the words which proceed it\n",
    "\n",
    "        args:\n",
    "            text (String) : The string of text you wish to train your markov model around\n",
    "\n",
    "        example:\n",
    "            text = 'hello my name is V hello my name is G hello my current name is F world today is a good day'\n",
    "            markov_model(text)\n",
    "            >> {'F': ['world'],\n",
    "                'G': ['hello'],\n",
    "                'V': ['hello'],\n",
    "                'a': ['good'],\n",
    "                'current': ['name'],\n",
    "                'good': ['day'],\n",
    "                'hello': ['my', 'my', 'my'],\n",
    "                'is': ['V', 'G', 'F', 'a'],\n",
    "                'my': ['name', 'name', 'current'],\n",
    "                'name': ['is', 'is', 'is'],\n",
    "                'today': ['is'],\n",
    "                'world': ['today']}\n",
    "        '''\n",
    "\n",
    "        # split the input text into individual words seperated by spaces\n",
    "        words = self.text.split(' ')\n",
    "\n",
    "        markov_dict = defaultdict(list)\n",
    "\n",
    "        # create list of all word pairs\n",
    "        for current_word, next_word in zip(words[0:-1], words[1:]):\n",
    "            markov_dict[current_word].append(next_word)\n",
    "\n",
    "        markov_dict = dict(markov_dict)\n",
    "        print('Successfully Trained')\n",
    "        return markov_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_words(chain, first_word, number_of_words=5):\n",
    "    '''\n",
    "    Given the input result from the markov_model function and the nunmber of words, this function will allow you to predict the next word\n",
    "    in the sequence\n",
    "    \n",
    "    args:\n",
    "        chain (Dictionary) : The result of the markov_model function\n",
    "        first_word (String) : The word you want to start your prediction from, note this word must be available in chain\n",
    "        number_of_words (Integer) : The number of words you want to predict\n",
    "    \n",
    "    example:\n",
    "        chain = markov_model(text)\n",
    "        generate_sentence(chain, first_word = 'do', number_of_words = 3)\n",
    "        >> Do not fail.\n",
    "    '''\n",
    "    \n",
    "    if first_word in list(chain.keys()):\n",
    "        word1 = str(first_word)\n",
    "        \n",
    "        predictions = word1.capitalize()\n",
    "\n",
    "        # Generate the second word from the value list. Set the new word as the first word. Repeat.\n",
    "        for i in range(number_of_words-1):\n",
    "            word2 = random.choice(chain[word1])\n",
    "            word1 = word2\n",
    "            predictions += ' ' + word2\n",
    "\n",
    "        # End it with a period\n",
    "        predictions += '.'\n",
    "        return predictions\n",
    "    else:\n",
    "        return \"Word not in corpus\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Trained\n",
      "Do not get one spot.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    m = Markov(file_path='eminem_songs_lyrics.txt')\n",
    "    chain = m.model\n",
    "    print(predict_words(chain, first_word = 'do'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "class MarkovModel:\n",
    "    \"\"\"\n",
    "    A simple discrete-time, discrete space first-order Markov model.\n",
    "    The probability matrix is a square matrix represented this way:\n",
    "    ```\n",
    "          +-----+-----+-----+\n",
    "          |  A  |  B  |  C  |\n",
    "    +-----+-----+-----+-----+\n",
    "    |  A  |  a  |  b  |  c  |\n",
    "    +-----+-----+-----+-----+\n",
    "    |  B  |  d  |  e  |  f  |\n",
    "    +-----+-----+-----+-----+\n",
    "    |  C  |  i  |  j  |  k  |\n",
    "    +-----+-----+-----+-----+\n",
    "    ```\n",
    "    with:\n",
    "     - `a` the probability for the state A to got to state A\n",
    "     - `b` the probability for the state A to got to state B\n",
    "     - `c` the probability for the state A to got to state C\n",
    "     - ...\n",
    "    Instead of using a 2D array, we use a dictionary of counters.\n",
    "    The dictionary contains the rows indexed by each state, each row contains counters indexed again by each state.\n",
    "    Using dictionary is usually simpler (we do not have to handle hash the elements), and faster than using an array\n",
    "    (O(1) instead of O(n) to access it, operation we use a lot).\n",
    "    Using a 2D array + a separate index + a hash function would be a bit faster, and a lot less memory consuming,\n",
    "    but more confusing and less generic.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, states):\n",
    "        \"\"\"\n",
    "        Create a markov chain\n",
    "        :param states: a set of all the different states\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "        # We create the matrix\n",
    "        self.matrix = {state: Counter() for state in self.states}\n",
    "\n",
    "    def next_state(self, current_state):\n",
    "        \"\"\"\n",
    "        Generate a next state according to the matrix's probabilities\n",
    "        :param current_state: the state to start with\n",
    "        :return: a next state\n",
    "        \"\"\"\n",
    "        row = self.matrix[current_state]  # We get the row associated with the current state\n",
    "\n",
    "        # Here, we want to get an random element in respect to the probabilities in the row. We do this in O(n) by\n",
    "        # selecting a random number between 0 and 1, walking though the elements and their probability in the list,\n",
    "        # subtracting the probabilities from our number until it is 0 or less.\n",
    "        # But since the probabilities in the row do not add up to 1 (it is only a part of the matrix), we generate a\n",
    "        # number between 0 and the sum of probabilities in the row\n",
    "        total = sum(row.values())\n",
    "        number = random.uniform(0.0, total)  # Generate a number in [0, total] with equal probability\n",
    "        for state, probability in row.items():\n",
    "            number -= probability\n",
    "            if number <= 0:\n",
    "                return state\n",
    "\n",
    "    def probability_of_chain(self, chain):\n",
    "        \"\"\"\n",
    "        Compute the probability for a given chain of text to occur.\n",
    "        :param chain: the chain of states as an ordered list\n",
    "        :return: the probability for it to happen\n",
    "        \"\"\"\n",
    "        # If the chain is empty, we return a null probability\n",
    "        if len(chain) == 0:\n",
    "            return 0\n",
    "\n",
    "        # If the chain is made of a single state, we return 1 if the state exists, 0 otherwise\n",
    "        if len(chain) == 1:\n",
    "            if chain[0] in self.matrix:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        probability = 1.0\n",
    "        for state, next_state in zip(chain, chain[1:]):\n",
    "            row = self.matrix[state]  # The row associated with the state\n",
    "\n",
    "            # If the transition between state and next_state is impossible, the probability of the chain is 0\n",
    "            if next_state not in row:\n",
    "                return 0\n",
    "\n",
    "            probability *= row[next_state]\n",
    "        return probability\n",
    "\n",
    "    def generate_chain(self, start_state, size):\n",
    "        \"\"\"\n",
    "        Generate of probable chain of state, respecting the probabilities in the matrix\n",
    "        :param start_state: the starting state of the chain\n",
    "        :param size: the size of the chain\n",
    "        :return: the chain as an ordered list\n",
    "        \"\"\"\n",
    "        chain = [start_state]\n",
    "        state = start_state\n",
    "        for n in range(0, size):\n",
    "            state = self.next_state(state)\n",
    "            chain.append(state)\n",
    "        return chain\n",
    "\n",
    "    def train(self, chain):\n",
    "        \"\"\"\n",
    "        Train the model on an example chain\n",
    "        :param chain: the chain of state as an ordered list\n",
    "        \"\"\"\n",
    "        # We read the text two words by two words\n",
    "        for s1, s2 in zip(chain, chain[1:]):\n",
    "            self.matrix[s1][s2] += 1\n",
    "\n",
    "        # We normalize the matrix, transforming occurrences into probabilities\n",
    "        factor = 1.0 / (len(chain) - 1)  # Instead of dividing by the number of words - 1, we use a multiplication\n",
    "        for row in self.matrix.values():\n",
    "            for state, occurences in row.items():\n",
    "                row[state] *= factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextMarkovModel(MarkovModel):\n",
    "    \"\"\"\n",
    "    A HMM that can be trained with a text and that is able to generate sentences from it.\n",
    "    Here the states are the words in the vocabulary of the text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text):\n",
    "        # We split the text into words\n",
    "        self.words = self._lex(text)\n",
    "        # The vocabulary is the set of different states\n",
    "        self.states = set(self.words)\n",
    "        super().__init__(self.states)\n",
    "\n",
    "    def train(self):\n",
    "        super().train(self.words)\n",
    "\n",
    "    def _lex(self, text):\n",
    "        \"\"\"\n",
    "        Splits the text into words\n",
    "        :param text: the text\n",
    "        :return: a list of words\n",
    "        \"\"\"\n",
    "        # Split at each character or sequence of character that is not a valid word character (in the \\w regex class)\n",
    "        return re.compile('[^\\w]+').split(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the two daughters of a most affectionate indulgent\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
    "and happy disposition, seemed to unite some of the best blessings\n",
    "of existence; and had lived nearly twenty-one years in the world\n",
    "with very little to distress or vex her.\n",
    "\n",
    "She was the youngest of the two daughters of a most affectionate,\n",
    "indulgent father; and had, in consequence of her sister's marriage,\n",
    "been mistress of his house from a very early period.  Her mother\n",
    "had died too long ago for her to have more than an indistinct\n",
    "remembrance of her caresses; and her place had been supplied\n",
    "by an excellent woman as governess, who had fallen little short\n",
    "of a mother in affection.\n",
    "\n",
    "Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
    "less as a governess than a friend, very fond of both daughters,\n",
    "but particularly of Emma.  Between _them_ it was more the intimacy\n",
    "of sisters.  Even before Miss Taylor had ceased to hold the nominal\n",
    "office of governess, the mildness of her temper had hardly allowed\n",
    "her to impose any restraint; and the shadow of authority being\n",
    "now long passed away, they had been living together as friend and\n",
    "friend very mutually attached, and Emma doing just what she liked;\n",
    "highly esteeming Miss Taylor's judgment, but directed chiefly by\n",
    "her own.\n",
    "'''\n",
    "hmm = TextMarkovModel(text)\n",
    "hmm.train()\n",
    "print(' '.join(hmm.generate_chain(\"the\", 7)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
