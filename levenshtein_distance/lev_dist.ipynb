{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39b01393-b3d5-45fb-9d78-7a5de09d261d",
   "metadata": {},
   "source": [
    "# Text Similarity - Levenshtein Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02846d60-7071-4e0c-823f-1dcc8e7f7325",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial I will show you the code necessary to implement Levenshtein distance in Python but for the actual text similarity pipeline we will be using the Levenshtein-python package. You can look through the installation guidelines [here](https://pypi.org/project/python-Levenshtein/) or run the following command:   \n",
    "```console\n",
    "pip install python-Levenshtein  \n",
    "```  \n",
    "\n",
    "### Introduction to Text Similarity\n",
    "Identifying similarity between text is a common problem in NLP and is used by many companies world wide. The most common application of text similarity comes from the form of identifying plagiarized text. Educational facilities ranging from elementary school, high school, college and universities all around the world use services like Turnitin to ensure the work submitted by students is original and their own. Other applications of text similarity is commonly used by companies which have a similar structure to Stack Overflow or Stack Exchange. They want to be able to identify and flag duplicated questions so the user posting the question can be referenced to the original post with the solution. This increases the number of unique questions being asked on their platform.  \n",
    "\n",
    "Text similarity can be broken down into two components, semantic similarity and lexical similarity. Given a pair of text, the semantic similarity of the pair refers to how close the documents are in meaning. Whereas, lexical similarity is a measure of overlap in vocabulary. If both documents in the pairs have the same vocabularies, then they would have a lexical similarity of 1 and vice versa of 0 if there was no overlap in vocabularies [2].    \n",
    "\n",
    "Achieving true semantic similarity is a very difficult and unsolved task in both NLP and Mathematics. It's a heavily researched area and a lot of the solutions proposed does involve a certain degree of lexical similarity in them. For the focuses of this article, I will not dive much deeper into semantic similarity, but focus a lot more on lexical similarity.  \n",
    "\n",
    "### Levenshtein Distance  \n",
    "There are many ways to identify the lexical similarities between a pair of text, the one which we'll be covering today in this article is Levenshtein distance. An algorithm invented in 1965 by Vladimir Levenshtein, a Soviet mathematician [1].  \n",
    "### Intuition  \n",
    ">Levenshtein distance is very impactful because it does not require two strings to be of equal length for them to be compared. >Intuitively speaking, Levenshtein distance is quite easy to understand.  \n",
    ">Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. [1]  \n",
    ">- https://en.wikipedia.org/wiki/Levenshtein_distance  \n",
    "\n",
    "Essentially implying that the output distance between the two is the cumulative sum of the single-character edits. The larger the output distance is implies that more changes were necessary to make the two words equal each other, and the lower the output distance is implies that fewer changes were necessary. For example, given a pair of words dream and dream the resulting Levenshtein distance would be 0 because the two words are the same. However, if the words were dream and steam the Levenshtein distance would be 2 as you would need to make 2 edits to change dr to st .\n",
    "Thus a large value for Levenshtein distance implies that the two documents were not similar, and a small value for the distance implies that the two documents were similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976c8181-eedd-47a0-a876-926994fa774a",
   "metadata": {},
   "source": [
    "## Implement Levenshtein Distance\n",
    "\n",
    "The Python code associated to implementing  Levenshtein distance using dynamic programming. The same code can be implemented through a brute force and iterative solution (be aware that the brute force solution would not be optimal in terms of time complexity).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1334183-ab3d-4955-815f-f95e3ac2d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d4c6ce4-f174-49e8-8d73-523120815eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lev_dist(a, b):\n",
    "    '''\n",
    "    This function will calculate the levenshtein distance between two input\n",
    "    strings a and b\n",
    "    \n",
    "    params:\n",
    "        a (String) : The first string you want to compare\n",
    "        b (String) : The second string you want to compare\n",
    "        \n",
    "    returns:\n",
    "        This function will return the distnace between string a and b.\n",
    "        \n",
    "    example:\n",
    "        a = 'stamp'\n",
    "        b = 'stomp'\n",
    "        lev_dist(a,b)\n",
    "        >> 1.0\n",
    "    '''\n",
    "    \n",
    "    @lru_cache(None)  # for memorization\n",
    "    def min_dist(s1, s2):\n",
    "\n",
    "        if s1 == len(a) or s2 == len(b):\n",
    "            return len(a) - s1 + len(b) - s2\n",
    "\n",
    "        # no change required\n",
    "        if a[s1] == b[s2]:\n",
    "            return min_dist(s1 + 1, s2 + 1)\n",
    "\n",
    "        return 1 + min(\n",
    "            min_dist(s1, s2 + 1),      # insert character\n",
    "            min_dist(s1 + 1, s2),      # delete character\n",
    "            min_dist(s1 + 1, s2 + 1),  # replace character\n",
    "        )\n",
    "\n",
    "    return min_dist(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89178ab5-29a7-4dab-b66f-b0daae1c1131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq1 = 'saturday'\n",
    "sq2 = 'sunday'\n",
    "lev_dist(sq1, sq2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6303c885-b03d-493e-9ba0-bc979b018593",
   "metadata": {},
   "source": [
    "For the purpose of comparing the user input article to another article, we will reference Wikipedia. You are able to fetch Wikipedia text data very easily through the Wikipedia library. You can run the following command on your console :   \n",
    "```pip3 install wikipedia```  \n",
    "or reference the installation documentation for this library [here](https://pypi.org/project/Wikipedia-API/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a665cad9-5979-4695-b2c5-2fbcf8a5e0df",
   "metadata": {},
   "source": [
    "## Text Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ccbc07b-eff4-496e-bb2c-5d4c64044832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import Levenshtein as lev\n",
    "import wikipedia\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc0123-f206-4c44-9579-56e488eb7f90",
   "metadata": {},
   "source": [
    "### Problem Statement \n",
    "Similar to softwares like Turnitin, we want to build a pipeline which identifies if an input article is plagiarized.   \n",
    "\n",
    "### Solution Architecture\n",
    "To solve this problem a few things will be required. Firstly, we need to get the information passed on by the user of the pipeline, for this we not only require the article that they want to check the plagiarism against but also a keyword tag which corresponds to that article. For the simplicity of this tutorial, we'll used the initial text I've written for this article with the tag being `Levenshtein Distance`. Second, we need a large corpus of documents we want to compare the user input text with. We can leverage the Wikipedia-API to get access to Wikipedia articles associated to the tag of the user input data.  We can then clean the user input document for redundancies like stopwords and punctuations to better optimize the calculation of Levenshtein distance. We pass this cleaned document through each document in our corpus under the same tag as the user input document and identify if there is any document which is very similar to the user submitted document.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79f7309-75fa-418b-9b64-d3fa3ef12069",
   "metadata": {},
   "source": [
    "## Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "547ade18-9f22-47e9-8df9-702f9b5ba7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_article = '''\n",
    "Identifying similarity between text is a common problem in NLP and is used by many companies world wide. The most common application of text similarity comes from the form of identifying plagiarized text. Educational facilities ranging from elementary school, high school, college and universities all around the world use services like Turnitin to ensure the work submitted by students is original and their own. Other applications of text similarity is commonly used by companies which have a similar structure to Stack Overflow or Stack Exchange. They want to be able to identify and flag duplicated questions so the user posting the question can be referenced to the original post with the solution. This increases the number of unique questions being asked on their platform. \n",
    "Text similarity can be broken down into two components, semantic similarity and lexical similarity. Given a pair of text, the semantic similarity of the pair refers to how close the documents are in meaning. Whereas, lexical similarity is a measure of overlap in vocabulary. If both documents in the pairs have the same vocabularies, then they would have a lexical similarity of 1 and vice versa of 0 if there was no overlap in vocabularies [2].\n",
    "Achieving true semantic similarity is a very difficult and unsolved task in both NLP and Mathematics. It's a heavily researched area and a lot of the solutions proposed does involve a certain degree of lexical similarity in them. For the focuses of this article, I will not dive much deeper into semantic similarity, but focus a lot more on lexical similarity.\n",
    "Levenshtein Distance\n",
    "There are many ways to identify the lexical similarities between a pair of text, the one which we'll be covering today in this article is Levenshtein distance. An algorithm invented in 1965 by Vladimir Levenshtein, a Soviet mathematician [1].\n",
    "Intuition\n",
    "Levenshtein distance is very impactful because it does not require two strings to be of equal length for them to be compared. Intuitively speaking, Levenshtein distance is quite easy to understand.\n",
    "Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. [1]\n",
    "- https://en.wikipedia.org/wiki/Levenshtein_distance\n",
    "Essentially implying that the output distance between the two is the cumulative sum of the single-character edits. The larger the output distance is implies that more changes were necessary to make the two words equal each other, and the lower the output distance is implies that fewer changes were necessary. For example, given a pair of words dream and dream the resulting Levenshtein distance would be 0 because the two words are the same. However, if the words were dream and steam the Levenshtein distance would be 2 as you would need to make 2 edits to change dr to st .\n",
    "Thus a large value for Levenshtein distance implies that the two documents were not similar, and a small value for the distance implies that the two documents were similar.\n",
    "'''\n",
    "\n",
    "tags = ['Levenshtein distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0aa7c51-c2d5-4704-942b-9dad1b1a54d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wiki_data(tags):\n",
    "    '''\n",
    "    The purpose of this function is to get the wikipedia data associated to a certain user\n",
    "    input tag.\n",
    "    \n",
    "    params:\n",
    "        tag (String) : The item you want to seach wikipedia for\n",
    "        \n",
    "    returns:\n",
    "        This function will return the contents associated to the user specified tag\n",
    "    \n",
    "    example:\n",
    "        tag = 'Levenshtein distance'\n",
    "        fetch_wiki_data(tag)\n",
    "        >> In information theory, linguistics, and computer science, the Levenshtein distance \n",
    "           is a string metric...\n",
    "    '''\n",
    "    content = {}\n",
    "    for tag in tags:\n",
    "        # get wikipedia data for the tag\n",
    "        wiki_tag = wikipedia.search(tag)\n",
    "\n",
    "        # get page info\n",
    "        page = wikipedia.page(wiki_tag[0])\n",
    "\n",
    "        # get page content\n",
    "        content[tag] = page.content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a7d2d1-21c8-49c0-afcf-f2578ef59c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 201 ms, sys: 44.5 ms, total: 245 ms\n",
      "Wall time: 3.76 s\n"
     ]
    }
   ],
   "source": [
    "%time tag_content = fetch_wiki_data(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fc79f7-f84f-4714-9776-0bd6a763e125",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d1da5ad-2745-4170-bed5-7be08590887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(txt, punct = string.punctuation):\n",
    "    '''\n",
    "    This function will remove punctuations from the input text\n",
    "    '''\n",
    "    return ''.join([c for c in txt if c not in punct])\n",
    "  \n",
    "def remove_stopwords(txt, sw = list(stopwords.words('english'))):\n",
    "    '''\n",
    "    This function will remove the stopwords from the input txt\n",
    "    '''\n",
    "    return ' '.join([w for w in txt.split() if w.lower() not in sw])\n",
    "\n",
    "def clean_text(txt):\n",
    "    '''\n",
    "    This function will clean the text being passed by removing specific line feed characters\n",
    "    like '\\n', '\\r', and '\\'\n",
    "    '''\n",
    "    \n",
    "    txt = txt.replace('\\n', ' ').replace('\\r', ' ').replace('\\'', '')\n",
    "    txt = remove_punctuations(txt)\n",
    "    txt = remove_stopwords(txt)\n",
    "    return txt.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54352deb-9bef-4038-a211-2181c3650a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.93 ms, sys: 514 µs, total: 5.44 ms\n",
      "Wall time: 6.59 ms\n"
     ]
    }
   ],
   "source": [
    "%time user_article = clean_text(user_article) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "581f4ee8-2aee-4fc7-a063-f96a42637616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.31 ms, sys: 1.5 ms, total: 9.8 ms\n",
      "Wall time: 9.96 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for tag, content in tag_content.items():\n",
    "    tag_content[tag] = clean_text(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2de531-35d2-4647-ad92-0c85b3061ff3",
   "metadata": {},
   "source": [
    "## Find Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86f7dd58-6e51-43b9-aaa8-c1bf2833ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(user_article, tag_content):\n",
    "    '''\n",
    "    This function will identify the similarities between the user_article and all the\n",
    "    content within tag_content\n",
    "    \n",
    "    params:\n",
    "        user_article (String) : The text submitted by the user\n",
    "        tag_content (Dictionary) : Key is the tag and the value is the content you want \n",
    "                                   to compare with the user_article\n",
    "    \n",
    "    returns:\n",
    "        This function will return a dictionary holding the Levenshtein assocaited to the \n",
    "        user_article with each tag_content\n",
    "    '''\n",
    "    \n",
    "    distances = {}\n",
    "    for tag,content in tag_content.items():\n",
    "        dist = lev.distance(user_article, content)\n",
    "        distances[tag] = dist\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3ea3ff6-e4d5-41b3-a5f8-c1463d30d13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Levenshtein distance': 4936}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = similarity(user_article, tag_content)\n",
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520bd164-9e47-47cb-9c54-ecfadd9c95dd",
   "metadata": {},
   "source": [
    "Now this value might seem relatively arbitrary to you, its hard to determine if this value reflects that the content is plagiarized or not. The larger the value is the less likely it is to be considered plagiarized based on our understanding of Levenshtein distance. However its difficult to determine that threshold of what distance is not large enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f8bd8-2343-4afe-9371-5cc5927f7771",
   "metadata": {},
   "source": [
    "## Check Plagiarism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9db0166-e796-43ee-b738-257b4a42f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_plagiarism(user_article, tag_content, distances, th = 0.4):\n",
    "    '''\n",
    "    This function will identify if the user_article is considered plagiarized for each\n",
    "    of the tag_content based on the distances observed.\n",
    "    \n",
    "    params:\n",
    "        user_article (String) : The text submitted by the user\n",
    "        tag_content (Dictionary) : Key is the tag and the value is the content you want \n",
    "                                   to compare with the user_article\n",
    "        distances (Dictionary) : Key is the tag and the value is the Levenshtein distance \n",
    "        th (Float) : The plagiarism threshold\n",
    "    \n",
    "    returns:\n",
    "        A dictionary associated to the plagiarism percentage for each tag\n",
    "    '''\n",
    "    ua_len = len(user_article)\n",
    "    distances = {tag:[d, max(ua_len, len(tag_content[tag]))] for tag,d in distances.items()}\n",
    "    \n",
    "    for tag, d in distances.items():\n",
    "        if d[0] <= d[1] * th:\n",
    "            distances[tag] = 'Plagiarized'\n",
    "        else:\n",
    "            distances[tag] = 'Not Plagiarized'\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5bc87b03-9ede-4235-a4ed-c5d7c531ca80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Levenshtein distance': 'Not Plagiarized'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_plagiarism(user_article, tag_content, distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af16d2d6-179e-47ce-b42d-e6e01c2dd808",
   "metadata": {},
   "source": [
    "## Caveats\n",
    "There are a number of caveats to using the pipeline outlined above. \n",
    "1) This pipeline does not identify which areas are plagiarized and which areas are not, it only yields an overall score of plagiarism.\n",
    "2) The process does not account of properly cited and quoted pieces of text. This would misleadingly increase the overall plagiarism score.\n",
    "3) It's difficult to determine a threshold of what how small or large a distance should be to be considered plagiarized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2759bb01-849d-422d-b57d-1b198150571e",
   "metadata": {},
   "source": [
    "## Concluding Remarks\n",
    "Levenshtein distance is a lexical similarity measure which identifies the distance between one a pair of strings. It does so by counting the number of times you would have to insert, delete or substitute a character from string 1 to make it like string 2. The larger the distance between the pair implies that the strings are not similar to each other and vice versa.  \n",
    "I created this pipeline in a manner such that its easily integratabtle with other text similarity measures. Levenshtein distance is a great measure to use to identify lexical similarity between a pair of text, but it does not mean there aren't other well performing similarity measures. The Jaro-Winkler score in particular comes to mind and can be easily implemented in this pipeline. Be aware that the Jaro similarity outputs a result which is interpreted differently than the Levenshtein distance.  \n",
    "You can follow through with this pipeline in the Jupyter Notebook I created for this project. You can find the notebook on my GitHub page [here](https://github.com/vatsal220/medium_articles/blob/main/levenshtein_distance/lev_dist.ipynb).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304f0779-9ace-4d44-840e-aa618b0984c7",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- [1] https://en.wikipedia.org/wiki/Levenshtein_distance\n",
    "- [2] https://en.wikipedia.org/wiki/Lexical_similarity\n",
    "- [3] https://pypi.org/project/python-Levenshtein/\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67d24198-e1a1-41ad-8ee5-90ecd8c19fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def levenshtein(seq1, seq2):\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "    print (matrix)\n",
    "    return (matrix[size_x - 1, size_y - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f695420-e02f-4763-a98a-a271b0415947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 2. 3. 4. 5.]\n",
      " [1. 0. 1. 2. 3. 4.]\n",
      " [2. 1. 0. 1. 2. 3.]\n",
      " [3. 2. 1. 1. 2. 3.]\n",
      " [4. 3. 2. 2. 1. 2.]\n",
      " [5. 4. 3. 3. 2. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'stamp'\n",
    "b = 'stomp'\n",
    "levenshtein(a,b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
