{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39b01393-b3d5-45fb-9d78-7a5de09d261d",
   "metadata": {},
   "source": [
    "# Text Similarity - Levenshtein Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02846d60-7071-4e0c-823f-1dcc8e7f7325",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial I will show you the code necessary to implement Levenshtein distance in Python but for the actual text similarity pipeline we will be using the Levenshtein-python package. You can look through the installation guidelines [here](https://pypi.org/project/python-Levenshtein/) or run the following command:   \n",
    "```console\n",
    "pip install python-Levenshtein  \n",
    "```  \n",
    "\n",
    "### Introduction to Text Similarity\n",
    "Identifying similarity between text is a common problem in NLP and is used by many companies world wide. The most common application of text similarity comes from the form of identifying plagiarized text. Educational facilities ranging from elementary school, high school, college and universities all around the world use services like Turnitin to ensure the work submitted by students is original and their own. Other applications of text similarity is commonly used by companies which have a similar structure to Stack Overflow or Stack Exchange. They want to be able to identify and flag duplicated questions so the user posting the question can be referenced to the original post with the solution. This increases the number of unique questions being asked on their platform.  \n",
    "\n",
    "Text similarity can be broken down into two components, semantic similarity and lexical similarity. Given a pair of text, the semantic similarity of the pair refers to how close the documents are in meaning. Whereas, lexical similarity is a measure of overlap in vocabulary. If both documents in the pairs have the same vocabularies, then they would have a lexical similarity of 1 and vice versa of 0 if there was no overlap in vocabularies [2].    \n",
    "\n",
    "Achieving true semantic similarity is a very difficult and unsolved task in both NLP and Mathematics. It's a heavily researched area and a lot of the solutions proposed does involve a certain degree of lexical similarity in them. For the focuses of this article, I will not dive much deeper into semantic similarity, but focus a lot more on lexical similarity.  \n",
    "\n",
    "### Levenshtein Distance  \n",
    "There are many ways to identify the lexical similarities between a pair of text, the one which we'll be covering today in this article is Levenshtein distance. An algorithm invented in 1965 by Vladimir Levenshtein, a Soviet mathematician [1].  \n",
    "### Intuition  \n",
    ">Levenshtein distance is very impactful because it does not require two strings to be of equal length for them to be compared. >Intuitively speaking, Levenshtein distance is quite easy to understand.  \n",
    ">Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. [1]  \n",
    ">- https://en.wikipedia.org/wiki/Levenshtein_distance  \n",
    "\n",
    "Essentially implying that the output distance between the two is the cumulative sum of the single-character edits. The larger the output distance is implies that more changes were necessary to make the two words equal each other, and the lower the output distance is implies that fewer changes were necessary. For example, given a pair of words dream and dream the resulting Levenshtein distance would be 0 because the two words are the same. However, if the words were dream and steam the Levenshtein distance would be 2 as you would need to make 2 edits to change dr to st .\n",
    "Thus a large value for Levenshtein distance implies that the two documents were not similar, and a small value for the distance implies that the two documents were similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976c8181-eedd-47a0-a876-926994fa774a",
   "metadata": {},
   "source": [
    "## Implement Levenshtein Distance\n",
    "\n",
    "The Python code associated to implementing  Levenshtein distance using dynamic programming. The same code can be implemented through a brute force and iterative solution (be aware that the brute force solution would not be optimal in terms of time complexity).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1334183-ab3d-4955-815f-f95e3ac2d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d4c6ce4-f174-49e8-8d73-523120815eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lev_dist(a, b):\n",
    "    '''\n",
    "    This function will calculate the levenshtein distance between two input\n",
    "    strings a and b\n",
    "    \n",
    "    params:\n",
    "        a (String) : The first string you want to compare\n",
    "        b (String) : The second string you want to compare\n",
    "        \n",
    "    returns:\n",
    "        This function will return the distnace between string a and b.\n",
    "        \n",
    "    example:\n",
    "        a = 'stamp'\n",
    "        b = 'stomp'\n",
    "        lev_dist(a,b)\n",
    "        >> 1.0\n",
    "    '''\n",
    "    \n",
    "    @lru_cache(None)  # for memorization\n",
    "    def min_dist(s1, s2):\n",
    "\n",
    "        if s1 == len(a) or s2 == len(b):\n",
    "            return len(a) - s1 + len(b) - s2\n",
    "\n",
    "        # no change required\n",
    "        if a[s1] == b[s2]:\n",
    "            return min_dist(s1 + 1, s2 + 1)\n",
    "\n",
    "        return 1 + min(\n",
    "            min_dist(s1, s2 + 1),      # insert character\n",
    "            min_dist(s1 + 1, s2),      # delete character\n",
    "            min_dist(s1 + 1, s2 + 1),  # replace character\n",
    "        )\n",
    "\n",
    "    return min_dist(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89178ab5-29a7-4dab-b66f-b0daae1c1131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq1 = 'saturday'\n",
    "sq2 = 'sunday'\n",
    "lev_dist(sq1, sq2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a665cad9-5979-4695-b2c5-2fbcf8a5e0df",
   "metadata": {},
   "source": [
    "## Text Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ccbc07b-eff4-496e-bb2c-5d4c64044832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import Levenshtein as lev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc0123-f206-4c44-9579-56e488eb7f90",
   "metadata": {},
   "source": [
    "### Problem Statement \n",
    "Similar to softwares like Turnitin, we want to build a pipeline which identifies if an input article is plagiarized.   \n",
    "\n",
    "### Solution Architecture\n",
    "To solve this problem we're going to make a large initial assumption. The assumption is that we have a large corpus of labelled documents which we want to cross reference with this particular user input document. We can then clean the user input document for redundancies like stopwords and punctuations to better optimize the calculation of Levenshtein distance. We pass this cleaned document through each document in our corpus under the same tag as the user input document and identify if there is any document which is very similar to the user submitted document.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79f7309-75fa-418b-9b64-d3fa3ef12069",
   "metadata": {},
   "source": [
    "## Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ade18-9f22-47e9-8df9-702f9b5ba7db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaafcfcf-e198-41b1-8da0-beea126af6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88fc79f7-f84f-4714-9776-0bd6a763e125",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04bbe3d-fa41-4b8a-819c-c345f4801108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606f2d9-2b5f-4a3d-b77b-6a97db153c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb2de531-35d2-4647-ad92-0c85b3061ff3",
   "metadata": {},
   "source": [
    "## Find Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea3ff6-e4d5-41b3-a5f8-c1463d30d13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f944c-6162-4096-9a3f-126ad73c1eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2759bb01-849d-422d-b57d-1b198150571e",
   "metadata": {},
   "source": [
    "## Concluding Remarks\n",
    "Levenshtein distance is a lexical similarity measure which identifies the distance between one a pair of strings. It does so by counting the number of times you would have to insert, delete or substitute a character from string 1 to make it like string 2. The larger the distance between the pair implies that the strings are not similar to each other and vice versa.  \n",
    "I created this pipeline in a manner such that its easily integratabtle with other text similarity measures. Levenshtein distance is a great measure to use to identify lexical similarity between a pair of text, but it does not mean there aren't other well performing similarity measures. The Jaro-Winkler score in particular comes to mind and can be easily implemented in this pipeline. Be aware that the Jaro similarity outputs a result which is interpreted differently than the Levenshtein distance.  \n",
    "You can follow through with this pipeline in the Jupyter Notebook I created for this project. You can find the notebook on my GitHub page [here](https://github.com/vatsal220/medium_articles/blob/main/levenshtein_distance/lev_dist.ipynb).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304f0779-9ace-4d44-840e-aa618b0984c7",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- [1] https://en.wikipedia.org/wiki/Levenshtein_distance\n",
    "- [2] https://en.wikipedia.org/wiki/Lexical_similarity\n",
    "- [3] https://pypi.org/project/python-Levenshtein/\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67d24198-e1a1-41ad-8ee5-90ecd8c19fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def levenshtein(seq1, seq2):\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "    print (matrix)\n",
    "    return (matrix[size_x - 1, size_y - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f695420-e02f-4763-a98a-a271b0415947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 2. 3. 4. 5.]\n",
      " [1. 0. 1. 2. 3. 4.]\n",
      " [2. 1. 0. 1. 2. 3.]\n",
      " [3. 2. 1. 1. 2. 3.]\n",
      " [4. 3. 2. 2. 1. 2.]\n",
      " [5. 4. 3. 3. 2. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'stamp'\n",
    "b = 'stomp'\n",
    "levenshtein(a,b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
